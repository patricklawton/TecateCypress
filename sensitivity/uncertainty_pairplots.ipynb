{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a8be4-81cd-4e3e-ba56-6f05f64ad1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colors, cm, rc\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import pickle\n",
    "import signac as sg\n",
    "from scipy.special import gamma\n",
    "import copy as copy\n",
    "import scipy\n",
    "from global_functions import adjustmaps\n",
    "import h5py\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import os\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Define/load things non-specific to a given set of results\n",
    "metric = 'lambda_s'\n",
    "Aeff = 7.29\n",
    "t_final = 300\n",
    "ncell_tot = 87_993\n",
    "c = 1.42\n",
    "with sg.H5Store('shared_data.h5').open(mode='r') as sd:\n",
    "    b_vec = np.array(sd['b_vec'])\n",
    "tau_vec = b_vec * gamma(1+1/c)\n",
    "tauc_methods = [\"flat\"]\n",
    "results_pre = 'gte_thresh' \n",
    "\n",
    "# Update global plotting parameters\n",
    "rc('axes', labelsize=21)  # Font size for x and y labels\n",
    "rc('axes', titlesize=16)\n",
    "rc('xtick', labelsize=19)  # Font size for x-axis tick labels\n",
    "rc('ytick', labelsize=19)  # Font size for y-axis tick labels\n",
    "rc('lines', markersize=15)  \n",
    "rc('lines', linewidth=5.5)\n",
    "rc('legend', fontsize=19)\n",
    "rc('font', family='sans-serif')\n",
    "rc('font', serif=['Computer Modern Sans Serif'] + plt.rcParams['font.serif'])\n",
    "rc('font', weight='light')\n",
    "histlw = 5.5\n",
    "cbar_lpad = 30\n",
    "dpi = 50\n",
    "# dpi = 200\n",
    "\n",
    "# Function to read in things specific to given results as global variables\n",
    "def set_globals(results_pre):\n",
    "    if metric == 'lambda_s':\n",
    "        globals()['metric_lab'] = r'$S$'\n",
    "        globals()['rob_metric_lab'] = r'$S^*$'\n",
    "        globals()['mean_metric_lab'] = r'$\\bar{\\lambda}(\\tau)$'\n",
    "    if metric == 'P_s':\n",
    "        globals()['metric_lab'] = r'$S_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\S_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<P_s>$'\n",
    "    if metric == 's':\n",
    "        globals()['metric_lab'] = r'$s_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\s_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<s>$'\n",
    "    globals()['fn_prefix'] = f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    # globals()['fig_prefix'] = f\"{results_pre}/figs/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    globals()['fig_prefix'] = os.path.join('/','Volumes', 'Macintosh HD', 'Users', 'patrick', \n",
    "                                           'Google Drive', 'My Drive', 'Research', 'Regan', 'Figs/')\n",
    "\n",
    "    # Load things saved specific to these results\n",
    "    globals()['metric_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/metric_all.npy\")\n",
    "    globals()['tau_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/tau_all.npy\")\n",
    "    globals()['C_vec'] = np.load(fn_prefix + \"C_vec.npy\")\n",
    "    globals()['C_i_vec'] = np.arange(len(C_vec))[::2]\n",
    "    globals()['ncell_vec'] = np.load(fn_prefix + \"ncell_vec.npy\")\n",
    "    globals()['slice_left_all'] = np.load(fn_prefix + \"slice_left_all.npy\")\n",
    "    eps_axes = {}\n",
    "    with h5py.File(fn_prefix + \"eps_axes.h5\", \"r\") as handle:\n",
    "        for key in handle.keys():\n",
    "            eps_axes.update({key: handle[key][()]})\n",
    "    globals()['eps_axes'] = eps_axes\n",
    "\n",
    "# Read in maps and convert fdm to tau, used by multiple plots below\n",
    "ul_coord = [1500, 2800]\n",
    "lr_coord = [2723, 3905]\n",
    "usecols = np.arange(ul_coord[0],lr_coord[0])\n",
    "sdmfn = \"../shared_maps/SDM_1995.asc\"\n",
    "sdm = np.loadtxt(sdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "fdmfn = '../shared_maps/FDE_current_allregions.asc'\n",
    "fdm = np.loadtxt(fdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "sdm, fdm = adjustmaps([sdm, fdm])\n",
    "delta_t = 30\n",
    "b_raster = delta_t / np.power(-np.log(1-fdm), 1/c)\n",
    "tau_raster = b_raster * gamma(1+1/c)\n",
    "maps_filt = (sdm > 0) & (fdm > 0)\n",
    "tau_flat = tau_raster[maps_filt] \n",
    "mapindices = np.argwhere(maps_filt)\n",
    "tau_argsort = np.argsort(tau_flat)\n",
    "tau_sorted = tau_flat[tau_argsort]\n",
    "\n",
    "set_globals(results_pre)\n",
    "phase = h5py.File(fn_prefix + '/phase.h5', 'r')\n",
    "\n",
    "meta_metric_nochange = float(np.load(fn_prefix + 'meta_metric_nochange.npy'))\n",
    "\n",
    "# Read in robustness data\n",
    "maxrob = np.load(fn_prefix + \"maxrob.npy\")\n",
    "argmaxrob = np.load(fn_prefix + \"argmaxrob.npy\")\n",
    "rob_thresh_vec = np.load(fn_prefix + \"rob_thresh_vec.npy\")\n",
    "rob_all = np.load(fn_prefix + \"rob_all.npy\")\n",
    "\n",
    "# Read in optimal decision params at baseline and uncertain conditions\n",
    "n_opt_baseline, l_opt_baseline = np.load(fn_prefix + 'decision_opt_baseline.npy')\n",
    "S_opt_baseline = np.load(fn_prefix + 'S_opt_baseline.npy')\n",
    "decision_opt_uncertain = np.load(fn_prefix + 'decision_opt_uncertain.npy')\n",
    "\n",
    "# Read in all splined interpolations of metric(tau)\n",
    "with open(fn_prefix + \"/metric_spl_all.pkl\", \"rb\") as handle:\n",
    "    metric_spl_all = pickle.load(handle)\n",
    "    \n",
    "# Get summarized information on lambda(tau) for all demographic samples\n",
    "# demographic_i_vec = np.arange(0, x_uncertain[:, 4].max() + 1, 1).astype(int)\n",
    "demographic_i_vec = np.arange(0, len(metric_spl_all), 1).astype(int)\n",
    "'''Hardcoding in 2 as min tau to match computations, update if needed'''\n",
    "tau_samples = np.linspace(2, tau_vec.max(), 100)\n",
    "baseline_lam = metric_spl_all[0](tau_samples)\n",
    "\n",
    "# We will summarize using difference in lambda from the baseline (i.e. average)\n",
    "lam_diff_vec = np.full(demographic_i_vec.size, np.nan)\n",
    "for demographic_i in demographic_i_vec:\n",
    "    # Compute the average difference in this sample's lambda from baseline across tau_samples\n",
    "    lam_diff = np.mean(metric_spl_all[demographic_i](tau_samples) - baseline_lam)\n",
    "    lam_diff_vec[demographic_i] = lam_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045527c-6d62-4f8c-940b-1cb736f38275",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Paramter pair plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cf98c-f1a3-49e7-aea6-dfc97177c3c9",
   "metadata": {},
   "source": [
    "##### updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdfcb8-eed0-4f22-8a31-b4af9cffd8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'mean_lam_diff']\n",
    "param_labels = [r'$\\mu_{\\tau}$', r'$\\sigma_{\\tau}$', r'$\\mu_{\\hat{\\tau}}$', \n",
    "                r'$\\sigma_{\\hat{\\tau}}$', r'$<\\lambda_m - \\bar{\\lambda}>$']\n",
    "\n",
    "# Get all possible parameter pairs\n",
    "param_pairs = [pair for pair in combinations(range(len(uncertain_params)), 2)]\n",
    "\n",
    "def get_pair_plots(C_i, n_i, l_i, Sstar, num_param_bins):\n",
    "    # Get the slice of range-wide stability at the specified deciison parameters\n",
    "    idx = \".\".join(str(i) for i in [C_i, n_i, l_i])\n",
    "    S_slice = np.array(phase[idx])\n",
    "    x_uncertain = np.array(phase[idx + 'uncertainty_samples'])\n",
    "\n",
    "    # Replace demographic samples in x_uncertain with our summary\n",
    "    x_uncertain[:, 4] = lam_diff_vec[x_uncertain[:,4].astype(int)]\n",
    "\n",
    "    # Preallocate filters for samples within bins of each parameter\n",
    "    bin_filts = {i: np.full((num_param_bins, x_uncertain.shape[0]), False) for i in range(x_uncertain.shape[-1])}\n",
    "    param_cntrs = {i: np.empty(num_param_bins) for i in range(x_uncertain.shape[-1])}\n",
    "    \n",
    "    for param_i in range(x_uncertain.shape[-1]):\n",
    "        # Bin the parameter along its sampled range\n",
    "        if uncertain_params[param_i] == 'mean_lam_diff':\n",
    "            '''There's an outlier making lam diff right skewed, take percentiles for now'''\n",
    "            param_low = np.percentile(lam_diff_vec, 0.1)\n",
    "            param_high = np.percentile(lam_diff_vec, 99.9)\n",
    "        else:\n",
    "            param_low = x_uncertain[:, param_i].min()\n",
    "            param_high = x_uncertain[:, param_i].max()\n",
    "        param_edges, step = np.linspace(param_low, param_high, num_param_bins + 1, retstep=True)\n",
    "\n",
    "        # Store bin centers for use in plotting\n",
    "        _param_cntrs = param_edges[:-1] + step/2\n",
    "        param_cntrs[param_i] = _param_cntrs\n",
    "\n",
    "        for bin_i, edge in enumerate(param_edges[:-1]):\n",
    "            _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "            bin_filts[param_i][bin_i] = _filt\n",
    "\n",
    "    # Initialize matrix to store results in\n",
    "    results = np.full((len(param_pairs), num_param_bins, num_param_bins), np.nan)\n",
    "    \n",
    "    # Loop over each bin for each parameter pair and compute a statistic on S for plotting\n",
    "    for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "        # Loop over bin combinations\n",
    "        bin_combinations = product(range(num_param_bins), range(num_param_bins))\n",
    "        for i, j in bin_combinations:\n",
    "            # Make filter for being within each bin\n",
    "            joint_filt = bin_filts[param_i][i] & bin_filts[param_j][j]\n",
    "\n",
    "            # Compute fraction of samples at or above Sstar within each joint parameter bin and store\n",
    "            if np.any(joint_filt):\n",
    "                # results[pair_i, i, j] = np.mean(S_slice[joint_filt])\n",
    "                results[pair_i, i, j] = np.count_nonzero(S_slice[joint_filt] >= Sstar) / np.count_nonzero(joint_filt)\n",
    "    \n",
    "    return results, param_cntrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe572d15-590b-47bc-a842-9bb6a26c1497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify number of bins for each uncertainty parameter\n",
    "num_param_bins = 13\n",
    "\n",
    "# Specify resource constraint\n",
    "C = 9 * ncell_tot\n",
    "\n",
    "# Get closest sampled Sstar to optimal S under baseline\n",
    "Sstar_i = np.abs(rob_thresh_vec - S_opt_baseline).argmin()\n",
    "\n",
    "all_results = {}\n",
    "for condition_key in ['baseline', 'uncertain']:\n",
    "    if condition_key == 'baseline':\n",
    "        n, l = (n_opt_baseline, l_opt_baseline)\n",
    "    else:\n",
    "        n, l = decision_opt_uncertain[Sstar_i, :]\n",
    "    print(f'optimal decision under {condition_key} conditions: (n,l) = {n, l}')\n",
    "\n",
    "    # Find their exact values in the decision parameter vectors\n",
    "    C_i = np.argmin(np.abs(C_vec - C))\n",
    "    C = C_vec[C_i]\n",
    "    n_i = np.argmin(np.abs(ncell_vec - n))\n",
    "    n = ncell_vec[n_i]\n",
    "    l_i = np.argmin(np.abs(slice_left_all - l))\n",
    "    l = slice_left_all[l_i]\n",
    "    \n",
    "    # Check if robustness is actually greater with robust optima, like I expect it to be\n",
    "    print(f'max robustness under {condition_key} optima: {rob_all[Sstar_i, C_i, n_i, l_i]}')\n",
    "\n",
    "    # Get results\n",
    "    results, param_cntrs = get_pair_plots(C_i, n_i, l_i, S_opt_baseline, num_param_bins)\n",
    "    all_results[condition_key] = results\n",
    "all_results['uncertain-baseline'] = all_results['uncertain'] - all_results['baseline']\n",
    "\n",
    "for condition_key in ['baseline', 'uncertain', 'uncertain-baseline']:\n",
    "    # Plot them\n",
    "    figdim = np.array([5,4])\n",
    "    fig, axes = plt.subplots(len(uncertain_params)-1, len(uncertain_params)-1, figsize=figdim*6)\n",
    "\n",
    "    for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "        results_pair = all_results[condition_key][pair_i]\n",
    "        \n",
    "        if condition_key == 'uncertain-baseline':\n",
    "            cmap = 'PuOr_r'\n",
    "            # Get extreme value of result for colorbar limits\n",
    "            extreme = max([np.abs(np.nanmin(results_pair)), np.nanmax(results_pair)])\n",
    "            norm = colors.TwoSlopeNorm(vmin=-extreme, vcenter=0, vmax=extreme)\n",
    "        else:\n",
    "            cmap = 'viridis'\n",
    "            norm = colors.Normalize(vmin=np.nanmin(results_pair), vmax=np.nanmax(results_pair))\n",
    "        \n",
    "        if param_i in [0, 2, 4]:\n",
    "            origin = 'upper'\n",
    "        else:\n",
    "            origin = 'lower'\n",
    "        \n",
    "        if param_j  in [0, 2, 4]:\n",
    "            results_pair = np.fliplr(results_pair.copy())\n",
    "            x_axis = np.flip(param_cntrs[param_j])\n",
    "        else:\n",
    "            x_axis = param_cntrs[param_j]\n",
    "        \n",
    "        im = axes[param_i,param_j-1].imshow(results_pair, origin=origin, norm=norm, cmap=cmap)\n",
    "        if param_j == len(uncertain_params) - 1:\n",
    "            label = r'$\\Delta P(S \\geq S^*)$' if condition_key == 'uncertain-baseline' else r'$P(S \\geq S^*)$'\n",
    "            cbar = fig.colorbar(im, shrink=0.75, label=label)\n",
    "        else:\n",
    "            cbar = fig.colorbar(im, shrink=0.75)\n",
    "        cbar.ax.tick_params(labelsize=plt.rcParams['axes.labelsize'] * 0.5)\n",
    "        tick_spacing = 1 if num_param_bins <= 5 else 2\n",
    "        axes[param_i,param_j-1].set_xlabel(param_labels[param_j], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "        axes[param_i,param_j-1].set_xticks(np.arange(num_param_bins)[::tick_spacing], \n",
    "                                           np.round(x_axis, 2)[::tick_spacing], \n",
    "                                           size=plt.rcParams['axes.labelsize']*0.25)\n",
    "        axes[param_i,param_j-1].set_ylabel(param_labels[param_i], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "        axes[param_i,param_j-1].set_yticks(np.arange(num_param_bins)[::tick_spacing], np.round(param_cntrs[param_i], 2)[::tick_spacing])\n",
    "        axes[param_i,param_j-1].tick_params(axis='both', labelsize=plt.rcParams['axes.labelsize'] * 0.75)\n",
    "        \n",
    "    # fig.savefig(fig_prefix + f'uncertainty_pairs_{condition_key}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03919c94-70c2-43e0-948a-983071558993",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460225c-a2e9-4b53-ac97-4eca1793b232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'mean_lam_diff']\n",
    "param_labels = [r'$\\mu_{\\tau}$', r'$\\sigma_{\\tau}$', r'$\\mu_{\\hat{\\tau}}$', \n",
    "                r'$\\sigma_{\\hat{\\tau}}$', r'$<\\lambda_m - \\bar{\\lambda}>$']\n",
    "\n",
    "# Get all possible parameter pairs\n",
    "param_pairs = [pair for pair in combinations(range(len(uncertain_params)), 2)]\n",
    "\n",
    "def get_pair_plots(C, n, l, Sstar, num_param_bins):\n",
    "    # Get the slice of range-wide stability at the above values\n",
    "    decision_filt = (x_all[:, 0] == C) & (x_all[:, 1] == n) & (x_all[:, 2] == l)\n",
    "    S_slice = meta_metric_all[decision_filt]\n",
    "    x_uncertain = x_all[decision_filt, 3:]\n",
    "\n",
    "    # Get summarized information on lambda(tau) for all demographic samples\n",
    "    demographic_i_vec = np.arange(0, x_uncertain[:, 4].max() + 1, 1).astype(int)\n",
    "    '''Hardcoding in 2 as min tau to match computations, update if needed'''\n",
    "    tau_samples = np.linspace(2, tau_vec.max(), 100)\n",
    "    baseline_lam = metric_spl_all[0](tau_samples)\n",
    "\n",
    "    # We will summarize using difference in lambda from the baseline (i.e. average)\n",
    "    lam_diff_vec = np.full(demographic_i_vec.size, np.nan)\n",
    "    for demographic_i in demographic_i_vec:\n",
    "        # Compute the average difference in this sample's lambda from baseline across tau_samples\n",
    "        lam_diff = np.mean(metric_spl_all[demographic_i](tau_samples) - baseline_lam)\n",
    "        lam_diff_vec[demographic_i] = lam_diff\n",
    "\n",
    "    # Replace demographic samples in x_uncertain with our summary\n",
    "    x_uncertain[:, 4] = lam_diff_vec[x_uncertain[:,4].astype(int)]\n",
    "\n",
    "    # Preallocate filters for samples within bins of each parameter\n",
    "    bin_filts = {i: np.full((num_param_bins, x_uncertain.shape[0]), False) for i in range(x_uncertain.shape[-1])}\n",
    "    param_cntrs = {i: np.empty(num_param_bins) for i in range(x_uncertain.shape[-1])}\n",
    "    \n",
    "    for param_i in range(x_uncertain.shape[-1]):\n",
    "        # Bin the parameter along its sampled range\n",
    "        if uncertain_params[param_i] == 'mean_lam_diff':\n",
    "            '''There's an outlier making lam diff right skewed, take percentiles for now'''\n",
    "            param_low = np.percentile(lam_diff_vec, 0.1)\n",
    "            param_high = np.percentile(lam_diff_vec, 99.9)\n",
    "        else:\n",
    "            param_low = x_uncertain[:, param_i].min()\n",
    "            param_high = x_uncertain[:, param_i].max()\n",
    "        param_edges, step = np.linspace(param_low, param_high, num_param_bins + 1, retstep=True)\n",
    "\n",
    "        # Store bin centers for use in plotting\n",
    "        _param_cntrs = param_edges[:-1] + step/2\n",
    "        param_cntrs[param_i] = _param_cntrs\n",
    "\n",
    "        for bin_i, edge in enumerate(param_edges[:-1]):\n",
    "            _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "            bin_filts[param_i][bin_i] = _filt\n",
    "\n",
    "    # Initialize matrix to store results in\n",
    "    results = np.full((len(param_pairs), num_param_bins, num_param_bins), np.nan)\n",
    "    \n",
    "    # Loop over each bin for each parameter pair and compute a statistic on S for plotting\n",
    "    for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "        # Loop over bin combinations\n",
    "        bin_combinations = product(range(num_param_bins), range(num_param_bins))\n",
    "        for i, j in bin_combinations:\n",
    "            # Make filter for being within each bin\n",
    "            joint_filt = bin_filts[param_i][i] & bin_filts[param_j][j]\n",
    "\n",
    "            # Compute fraction of samples at or above Sstar within each joint parameter bin and store\n",
    "            if np.any(joint_filt):\n",
    "                # results[pair_i, i, j] = np.mean(S_slice[joint_filt])\n",
    "                results[pair_i, i, j] = np.count_nonzero(S_slice[joint_filt] >= Sstar) / np.count_nonzero(joint_filt)\n",
    "    \n",
    "    return results, param_cntrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f126e80-3298-468c-bb76-54e3e0249c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify number of bins for each uncertainty parameter\n",
    "num_param_bins = 13\n",
    "\n",
    "# Specify resource constraint\n",
    "C = 9 * ncell_tot\n",
    "\n",
    "# Get closest sampled Sstar to optimal S under baseline\n",
    "Sstar_i = np.abs(rob_thresh_vec - S_opt_baseline).argmin()\n",
    "\n",
    "all_results = {}\n",
    "for condition_key in ['baseline', 'uncertain']:\n",
    "    if condition_key == 'baseline':\n",
    "        n, l = (n_opt_baseline, l_opt_baseline)\n",
    "    else:\n",
    "        n, l = decision_opt_uncertain[Sstar_i, :]\n",
    "    print(f'optimal decision under {condition_key} conditions: (n,l) = {n, l}')\n",
    "\n",
    "    # Find their exact values in the decision parameter vectors\n",
    "    C_i = np.argmin(np.abs(C_vec - C))\n",
    "    C = C_vec[C_i]\n",
    "    n_i = np.argmin(np.abs(ncell_vec - n))\n",
    "    n = ncell_vec[n_i]\n",
    "    l_i = np.argmin(np.abs(slice_left_all - l))\n",
    "    l = slice_left_all[l_i]\n",
    "    \n",
    "    # Check if robustness is actually greater with robust optima, like I expect it to be\n",
    "    print(f'max robustness under {condition_key} optima: {rob_all[Sstar_i, C_i, n_i, l_i]}')\n",
    "\n",
    "    # Get results\n",
    "    results, param_cntrs = get_pair_plots(C, n, l, S_opt_baseline, num_param_bins)\n",
    "    all_results[condition_key] = results\n",
    "all_results['uncertain-baseline'] = all_results['uncertain'] - all_results['baseline']\n",
    "\n",
    "for condition_key in ['baseline', 'uncertain', 'uncertain-baseline']:\n",
    "    # Plot them\n",
    "    figdim = np.array([5,4])\n",
    "    fig, axes = plt.subplots(len(uncertain_params)-1, len(uncertain_params)-1, figsize=figdim*6)\n",
    "\n",
    "    for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "        results_pair = all_results[condition_key][pair_i]\n",
    "        \n",
    "        if condition_key == 'uncertain-baseline':\n",
    "            cmap = 'PuOr_r'\n",
    "            # Get extreme value of result for colorbar limits\n",
    "            extreme = max([np.abs(np.nanmin(results_pair)), np.nanmax(results_pair)])\n",
    "            norm = colors.TwoSlopeNorm(vmin=-extreme, vcenter=0, vmax=extreme)\n",
    "        else:\n",
    "            cmap = 'viridis'\n",
    "            norm = colors.Normalize(vmin=np.nanmin(results_pair), vmax=np.nanmax(results_pair))\n",
    "        \n",
    "        if param_i in [0, 2, 4]:\n",
    "            origin = 'upper'\n",
    "        else:\n",
    "            origin = 'lower'\n",
    "        \n",
    "        if param_j  in [0, 2, 4]:\n",
    "            results_pair = np.fliplr(results_pair.copy())\n",
    "            x_axis = np.flip(param_cntrs[param_j])\n",
    "        else:\n",
    "            x_axis = param_cntrs[param_j]\n",
    "        \n",
    "        im = axes[param_i,param_j-1].imshow(results_pair, origin=origin, norm=norm, cmap=cmap)\n",
    "        if param_j == len(uncertain_params) - 1:\n",
    "            label = r'$\\Delta P(S \\geq S^*)$' if condition_key == 'uncertain-baseline' else r'$P(S \\geq S^*)$'\n",
    "            cbar = fig.colorbar(im, shrink=0.75, label=label)\n",
    "        else:\n",
    "            cbar = fig.colorbar(im, shrink=0.75)\n",
    "        cbar.ax.tick_params(labelsize=plt.rcParams['axes.labelsize'] * 0.5)\n",
    "        tick_spacing = 1 if num_param_bins <= 5 else 2\n",
    "        axes[param_i,param_j-1].set_xlabel(param_labels[param_j], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "        axes[param_i,param_j-1].set_xticks(np.arange(num_param_bins)[::tick_spacing], \n",
    "                                           np.round(x_axis, 2)[::tick_spacing], \n",
    "                                           size=plt.rcParams['axes.labelsize']*0.25)\n",
    "        axes[param_i,param_j-1].set_ylabel(param_labels[param_i], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "        axes[param_i,param_j-1].set_yticks(np.arange(num_param_bins)[::tick_spacing], np.round(param_cntrs[param_i], 2)[::tick_spacing])\n",
    "        axes[param_i,param_j-1].tick_params(axis='both', labelsize=plt.rcParams['axes.labelsize'] * 0.75)\n",
    "        \n",
    "    fig.savefig(fig_prefix + f'uncertainty_pairs_{condition_key}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a3557-5fb2-42db-a74c-e9d7b3b78d39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### First pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1de3f-e904-4ada-a386-72d8c295701d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'mean_lam_diff']\n",
    "param_labels = [r'$\\mu_{\\tau}$', r'$\\sigma_{\\tau}$', r'$\\mu_{\\hat{\\tau}}$', \n",
    "                r'$\\sigma_{\\hat{\\tau}}$', r'$<\\lambda_m - \\bar{\\lambda}>$']\n",
    "\n",
    "# Specify resource constraint and decision parameters\n",
    "C = 9 * ncell_tot\n",
    "# n, l = (55_000, 22_000)\n",
    "condition_key = 'baseline'\n",
    "assert condition_key in ['baseline', 'uncertain']\n",
    "if condition_key == 'baseline':\n",
    "    n, l = (n_opt_baseline, l_opt_baseline)\n",
    "else:\n",
    "    Sstar_i = np.abs(rob_thresh_vec - S_opt_baseline).argmin()\n",
    "    n, l = decision_opt_uncertain[Sstar_i, :]\n",
    "\n",
    "# Find their exact values in the decision parameter vectors\n",
    "C = C_vec[np.argmin(np.abs(C_vec - C))]\n",
    "n = ncell_vec[np.argmin(np.abs(ncell_vec - n))]\n",
    "l = slice_left_all[np.argmin(np.abs(slice_left_all - l))]\n",
    "\n",
    "# Get the slice of range-wide stability at the above values\n",
    "decision_filt = (x_all[:, 0] == C) & (x_all[:, 1] == n) & (x_all[:, 2] == l)\n",
    "S_slice = meta_metric_all[decision_filt]\n",
    "x_uncertain = x_all[decision_filt, 3:]\n",
    "\n",
    "# Read in all splined interpolations of metric(tau)\n",
    "with open(fn_prefix + \"/metric_spl_all.pkl\", \"rb\") as handle:\n",
    "    metric_spl_all = pickle.load(handle)\n",
    "\n",
    "# Get summarized information on lambda(tau) for all demographic samples\n",
    "demographic_i_vec = np.arange(0, x_uncertain[:, 4].max() + 1, 1).astype(int)\n",
    "'''Hardcoding in 2 as min tau to match computations, update if needed'''\n",
    "tau_samples = np.linspace(2, tau_vec.max(), 100)\n",
    "baseline_lam = metric_spl_all[0](tau_samples)\n",
    "\n",
    "# We will summarize using difference in lambda from the baseline (i.e. average)\n",
    "lam_diff_vec = np.full(demographic_i_vec.size, np.nan)\n",
    "for demographic_i in demographic_i_vec:\n",
    "    # Compute the average difference in this sample's lambda from baseline across tau_samples\n",
    "    lam_diff = np.mean(metric_spl_all[demographic_i](tau_samples) - baseline_lam)\n",
    "    lam_diff_vec[demographic_i] = lam_diff\n",
    "    \n",
    "# Replace demographic samples in x_uncertaint with our summary\n",
    "x_uncertain[:, 4] = lam_diff_vec[x_uncertain[:,4].astype(int)]\n",
    "\n",
    "# Preallocate filters for samples within bins of each parameter\n",
    "num_param_bins = 10\n",
    "bin_filts = {i: np.full((num_param_bins, x_uncertain.shape[0]), False) for i in range(x_uncertain.shape[-1])}\n",
    "param_cntrs = {i: np.empty(num_param_bins) for i in range(x_uncertain.shape[-1])}\n",
    "\n",
    "for param_i in range(x_uncertain.shape[-1]):\n",
    "    # Bin the parameter along its sampled range\n",
    "    if uncertain_params[param_i] == 'mean_lam_diff':\n",
    "        '''There's an outlier making lam diff right skewed, take percentiles for now'''\n",
    "        param_low = np.percentile(lam_diff_vec, 0.1)\n",
    "        param_high = np.percentile(lam_diff_vec, 99.9)\n",
    "    else:\n",
    "        param_low = x_uncertain[:, param_i].min()\n",
    "        param_high = x_uncertain[:, param_i].max()\n",
    "    param_edges, step = np.linspace(param_low, param_high, num_param_bins + 1, retstep=True)\n",
    "    \n",
    "    # Store bin centers for use in plotting\n",
    "    _param_cntrs = param_edges[:-1] + step/2\n",
    "    param_cntrs[param_i] = _param_cntrs\n",
    "    \n",
    "    for bin_i, edge in enumerate(param_edges[:-1]):\n",
    "        _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "        bin_filts[param_i][bin_i] = _filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b778a69-2029-4c80-bfed-0f654706482e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figdim = np.array([5,4])\n",
    "# fig, axes = plt.subplots(len(uncertain_params) - 1, len(uncertain_params) - 1, figsize=figdim*10)\n",
    "fig, axes = plt.subplots(len(uncertain_params)-1, len(uncertain_params)-1, figsize=figdim*8)\n",
    "\n",
    "# Loop over parameter pairs\n",
    "from itertools import combinations\n",
    "param_pairs = combinations(range(len(uncertain_params)), 2)\n",
    "for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "    # Initialize matrix to store results in\n",
    "    results = np.full((num_param_bins, num_param_bins), np.nan)\n",
    "    \n",
    "    # Loop over bin combinations\n",
    "    bin_combinations = product(range(num_param_bins), range(num_param_bins))\n",
    "    for i, j in bin_combinations:\n",
    "        # Make filter for being within each bin\n",
    "        joint_filt = bin_filts[param_i][i] & bin_filts[param_j][j]\n",
    "        \n",
    "        # Compute average S at each joint parameter bin and store\n",
    "        if np.any(joint_filt):\n",
    "            results[i, j] = np.mean(S_slice[joint_filt])\n",
    "    \n",
    "    im = axes[param_i,param_j-1].imshow(results, origin='upper')\n",
    "    fig.colorbar(im, shrink=1, label=r'$S$')\n",
    "    tick_spacing = 2\n",
    "    axes[param_i,param_j-1].set_xlabel(param_labels[param_j])\n",
    "    axes[param_i,param_j-1].set_xticks(np.arange(num_param_bins)[::tick_spacing], np.round(param_cntrs[param_j], 2)[::tick_spacing])\n",
    "    axes[param_i,param_j-1].set_ylabel(param_labels[param_i])\n",
    "    axes[param_i,param_j-1].set_yticks(np.arange(num_param_bins)[::tick_spacing], np.round(param_cntrs[param_i], 2)[::tick_spacing])\n",
    "    \n",
    "fig.savefig(fig_prefix + f'uncertainty_pairs_{condition_key}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d74239-e437-454a-af0e-317d4eacaf5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Single parameter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27c71c-d8af-4b26-aa8b-7eba0bfadf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'demographic_index']\n",
    "\n",
    "# Specify resource constraint and decision parameters\n",
    "C = 9 * ncell_tot\n",
    "n, l = (55_000, 22_000)\n",
    "\n",
    "# Find their exact values in the decision parameter vectors\n",
    "C = C_vec[np.argmin(np.abs(C_vec - C))]\n",
    "n = ncell_vec[np.argmin(np.abs(ncell_vec - n))]\n",
    "l = slice_left_all[np.argmin(np.abs(slice_left_all - l))]\n",
    "\n",
    "# Get the slice of range-wide stability at the above values\n",
    "decision_filt = (x_all[:, 0] == C) & (x_all[:, 1] == n) & (x_all[:, 2] == l)\n",
    "S_slice = meta_metric_all[decision_filt]\n",
    "x_uncertain = x_all[decision_filt, 3:]\n",
    "\n",
    "for param_i in range(x_uncertain.shape[-1]):\n",
    "    # Bin the parameter along its sampled range\n",
    "    num_param_bins = 10\n",
    "    param_low = x_uncertain[:, param_i].min()\n",
    "    param_high = x_uncertain[:, param_i].max()\n",
    "    param_edges, step = np.linspace(param_low, param_high, num_param_bins, retstep=True)\n",
    "    param_cntrs = param_edges[:-1] + step/2\n",
    "    \n",
    "    # Get the means and upper/lower percentiles of S within each bin\n",
    "    S_means = np.empty_like(param_cntrs)\n",
    "    errors = np.empty((2, len(param_cntrs)))\n",
    "    for cntr_i, edge in enumerate(param_edges[:-1]):\n",
    "        _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "        S_means[cntr_i] = np.mean(S_slice[_filt])\n",
    "        errors[0, cntr_i] = np.percentile(S_slice[_filt], 25)\n",
    "        errors[1, cntr_i] = np.percentile(S_slice[_filt], 75)\n",
    "    \n",
    "    # Plot\n",
    "    plt.errorbar(param_cntrs, S_means, yerr=errors, marker='o', label=uncertain_params[param_i])\n",
    "    plt.ylim(S_slice.min(), S_slice.max())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
