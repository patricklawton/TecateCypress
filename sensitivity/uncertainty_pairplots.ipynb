{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f34af7-f962-4e6f-ae0f-5f5233de774a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colors, cm, rc\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import signac as sg\n",
    "from scipy.special import gamma\n",
    "import copy as copy\n",
    "from global_functions import adjustmaps\n",
    "import h5py\n",
    "from itertools import product, combinations\n",
    "\n",
    "# Define/load things non-specific to a given set of results\n",
    "metric = 'lambda_s'\n",
    "Aeff = 7.29\n",
    "t_final = 300\n",
    "ncell_tot = 87_993\n",
    "c = 1.42\n",
    "with sg.H5Store('shared_data.h5').open(mode='r') as sd:\n",
    "    b_vec = np.array(sd['b_vec'])\n",
    "tau_vec = b_vec * gamma(1+1/c)\n",
    "tauc_methods = [\"flat\"]\n",
    "results_pre = 'gte_thresh'\n",
    "\n",
    "# Update global plotting parameters\n",
    "rc('axes', labelsize=21)  # Font size for x and y labels\n",
    "rc('axes', titlesize=16)\n",
    "rc('xtick', labelsize=19)  # Font size for x-axis tick labels\n",
    "rc('ytick', labelsize=19)  # Font size for y-axis tick labels\n",
    "rc('lines', markersize=15)\n",
    "rc('lines', linewidth=5.5)\n",
    "rc('legend', fontsize=19)\n",
    "rc('font', family='sans-serif')\n",
    "rc('font', serif=['Computer Modern Sans Serif'] + plt.rcParams['font.serif'])\n",
    "rc('font', weight='light')\n",
    "histlw = 5.5\n",
    "cbar_lpad = 30\n",
    "dpi = 50\n",
    "\n",
    "# Function to read in things specific to given results as global variables\n",
    "def set_globals(results_pre):\n",
    "    if metric == 'lambda_s':\n",
    "        globals()['metric_lab'] = r'$S$'\n",
    "        globals()['rob_metric_lab'] = r'$S^*$'\n",
    "        globals()['mean_metric_lab'] = r'$\\bar{\\lambda}(\\tau)$'\n",
    "    if metric == 'P_s':\n",
    "        globals()['metric_lab'] = r'$S_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\S_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<P_s>$'\n",
    "    if metric == 's':\n",
    "        globals()['metric_lab'] = r'$s_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\s_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<s>$'\n",
    "    globals()['fn_prefix'] = f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    globals()['fig_prefix'] = f\"{results_pre}/figs/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    #globals()['fig_prefix'] = os.path.join('/','Volumes', 'Macintosh HD', 'Users', 'patrick',\n",
    "    #                                       'Google Drive', 'My Drive', 'Research', 'Regan', 'Figs/')\n",
    "\n",
    "# Read in maps and convert fdm to tau, used by multiple plots below\n",
    "ul_coord = [1500, 2800]\n",
    "lr_coord = [2723, 3905]\n",
    "usecols = np.arange(ul_coord[0],lr_coord[0])\n",
    "sdmfn = \"../shared_maps/SDM_1995.asc\"\n",
    "sdm = np.loadtxt(sdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "fdmfn = '../shared_maps/FDE_current_allregions.asc'\n",
    "fdm = np.loadtxt(fdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "sdm, fdm = adjustmaps([sdm, fdm])\n",
    "delta_t = 30\n",
    "b_raster = delta_t / np.power(-np.log(1-fdm), 1/c)\n",
    "tau_raster = b_raster * gamma(1+1/c)\n",
    "maps_filt = (sdm > 0) & (fdm > 0)\n",
    "tau_flat = tau_raster[maps_filt]\n",
    "mapindices = np.argwhere(maps_filt)\n",
    "tau_argsort = np.argsort(tau_flat)\n",
    "tau_sorted = tau_flat[tau_argsort]\n",
    "\n",
    "# Define keys and labels for parameters\n",
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'mean_lam_diff']\n",
    "param_labels = [r'$\\mu_{\\tau}$', r'$\\sigma_{\\tau}$', r'$\\mu_{\\hat{\\tau}}$', \n",
    "                r'$\\sigma_{\\hat{\\tau}}$', r'$<\\lambda_m - \\bar{\\lambda}>$']\n",
    "\n",
    "# Get all possible parameter pairs\n",
    "param_pairs = [pair for pair in combinations(range(len(uncertain_params)), 2)]\n",
    "\n",
    "# Read in data of S samples at optimal decisions, as well as some other things\n",
    "set_globals(results_pre)\n",
    "phase = h5py.File(fn_prefix + '/phase_optdecisions.h5', 'r')\n",
    "meta_metric_nochange = float(np.load(fn_prefix + 'meta_metric_nochange.npy'))\n",
    "rob_thresh_vec = np.load(fn_prefix + \"rob_thresh_vec.npy\")\n",
    "\n",
    "# Read in optimal S under baseline conditions\n",
    "S_opt_baseline = np.load(fn_prefix + 'S_opt_baseline.npy')\n",
    "# I don't trust interpolation of robustness between Sstar values, \n",
    "# so use the closest one we sampled for interpolator fitting\n",
    "'''This probably isnt necessary'''\n",
    "Sstar_i = np.argmin(np.abs(rob_thresh_vec - S_opt_baseline) )\n",
    "S_opt_baseline = rob_thresh_vec[Sstar_i]\n",
    "\n",
    "# Read in optimal decision params at baseline and uncertain conditions\n",
    "q_vec = np.load(fn_prefix + 'q_vec.npy')\n",
    "Sstar_i_optdecisions = np.load(fn_prefix + 'Sstar_i_optdecisions.npy')\n",
    "ncell_vec = np.load(fn_prefix + 'ncell_vec_optdecisions.npy')\n",
    "slice_left_all = np.load(fn_prefix + 'slice_left_all_optdecisions.npy')\n",
    "C_vec = np.load(fn_prefix + 'C_vec_optdecisions.npy')\n",
    "\n",
    "# Read in all splined interpolations of metric(tau)\n",
    "with open(fn_prefix + \"/metric_spl_all.pkl\", \"rb\") as handle:\n",
    "    metric_spl_all = pickle.load(handle)\n",
    "\n",
    "# Summarize uncertainty in demography by average difference in lambda_m from baseline lambda\n",
    "demographic_i_vec = np.arange(0, len(metric_spl_all), 1).astype(int)\n",
    "'''Hardcoding in 2 as min tau to match computations, update if needed'''\n",
    "tau_samples = np.linspace(2, tau_vec.max(), 100)\n",
    "baseline_lam = metric_spl_all[0](tau_samples)\n",
    "lam_diff_vec = np.full(demographic_i_vec.size, np.nan)\n",
    "for demographic_i in demographic_i_vec:\n",
    "    # Compute the average difference in this sample's lambda from baseline across tau_samples\n",
    "    lam_diff = np.mean(metric_spl_all[demographic_i](tau_samples) - baseline_lam)\n",
    "    lam_diff_vec[demographic_i] = lam_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214af64-64eb-42e6-9107-bd48087f8a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pair_results(C_i, n_i, l_i, Sstar, num_param_bins):\n",
    "    # Get the slice of range-wide stability at the specified decision parameters\n",
    "    idx = \".\".join(str(i) for i in [C_i, n_i, l_i])\n",
    "    S_slice = np.array(phase[idx])\n",
    "    x_uncertain = np.array(phase[idx + 'uncertainty_samples'])\n",
    "    \n",
    "    # Compute robustness\n",
    "    counts = np.count_nonzero(S_slice >= Sstar)\n",
    "    robustness = counts / len(S_slice)\n",
    "    print(f'robustness={robustness}')\n",
    "    \n",
    "    # Compute a normalizing factor to compare parameter bins\n",
    "    norm_factor = robustness * num_param_bins**2\n",
    "\n",
    "    # Replace demographic samples in x_uncertain with our summary\n",
    "    x_uncertain[:, 4] = lam_diff_vec[x_uncertain[:,4].astype(int)]\n",
    "\n",
    "    # Preallocate filters for samples within bins of each parameter\n",
    "    bin_filts = {i: np.full((num_param_bins, x_uncertain.shape[0]), False) for i in range(x_uncertain.shape[-1])}\n",
    "    param_cntrs = {i: np.empty(num_param_bins) for i in range(x_uncertain.shape[-1])}\n",
    "\n",
    "    for param_i in range(x_uncertain.shape[-1]):\n",
    "        # Bin the parameter along its sampled range\n",
    "        if uncertain_params[param_i] == 'mean_lam_diff':\n",
    "            '''There's an outlier making lam diff right skewed, take percentiles for now'''\n",
    "            param_low = np.percentile(lam_diff_vec, 0.1)\n",
    "            param_high = np.percentile(lam_diff_vec, 99.9)\n",
    "        else:\n",
    "            param_low = x_uncertain[:, param_i].min()\n",
    "            param_high = x_uncertain[:, param_i].max()\n",
    "        param_edges, step = np.linspace(param_low, param_high, num_param_bins + 1, retstep=True)\n",
    "\n",
    "        # Store bin centers for use in plotting\n",
    "        _param_cntrs = param_edges[:-1] + step/2\n",
    "        param_cntrs[param_i] = _param_cntrs\n",
    "        for bin_i, edge in enumerate(param_edges[:-1]):\n",
    "            _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "            bin_filts[param_i][bin_i] = _filt\n",
    "\n",
    "    # Initialize matrix to store results in\n",
    "    results = np.full((len(param_pairs), num_param_bins, num_param_bins), np.nan)\n",
    "\n",
    "    # Loop over each bin for each parameter pair and compute a statistic on S \n",
    "    for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "        # Loop over bin combinations\n",
    "        bin_combinations = product(range(num_param_bins), range(num_param_bins))\n",
    "        for i, j in bin_combinations:\n",
    "            # Make filter for being within each bin\n",
    "            joint_filt = bin_filts[param_i][i] & bin_filts[param_j][j]\n",
    "\n",
    "            # Compute fraction of samples at or above Sstar (i.e. the probability of our target being met) \n",
    "            # within each joint parameter bin use that to compute bin's contribution to the robustness\n",
    "            if np.any(joint_filt):\n",
    "                P_targetmet = np.count_nonzero(S_slice[joint_filt] >= Sstar) / np.count_nonzero(joint_filt)\n",
    "                # We need to normalize to make these comparisons\n",
    "                results[pair_i, i, j] = P_targetmet / norm_factor\n",
    "                '''Or just look at P_targetmet'''\n",
    "                # results[pair_i, i, j] = P_targetmet\n",
    "                '''Or just divide counts in this bin by total counts'''\n",
    "                results[pair_i, i, j] = np.count_nonzero(S_slice[joint_filt] >= Sstar) / counts\n",
    "\n",
    "    return results, param_cntrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dd8a0-eb9e-4133-863b-7cb3c7dea436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify number of bins for each uncertainty parameter\n",
    "num_param_bins = 13\n",
    "\n",
    "# Specify resource constraint\n",
    "C = 10 * ncell_tot\n",
    "assert C in C_vec\n",
    "C_i = np.argmin(np.abs(C_vec - C))\n",
    "print(f'C/n_tot={C_vec[C_i]/ncell_tot}')\n",
    "\n",
    "# Get results under baseline conditions\n",
    "all_results = {}\n",
    "results, param_cntrs = get_pair_results(0, 0, 0, S_opt_baseline, num_param_bins)\n",
    "all_results['baseline'] = results\n",
    "\n",
    "# Specify what percent decrease in optimal S baseline we want to look at\n",
    "for q_i in range(q_vec.size):\n",
    "    # Get corresponding Sstar\n",
    "    Sstar_i = Sstar_i_optdecisions[q_i]\n",
    "    Sstar = rob_thresh_vec[Sstar_i]\n",
    "\n",
    "    # Get results at the specified q value for comparison to baseline\n",
    "    results, param_cntrs = get_pair_results(C_i, q_i+1, q_i+1, Sstar, num_param_bins)\n",
    "    all_results['uncertain'] = results\n",
    "\n",
    "    # Take difference between results\n",
    "    all_results['uncertain-baseline'] = all_results['uncertain'] - all_results['baseline']\n",
    "\n",
    "    for condition_key in ['baseline', 'uncertain', 'uncertain-baseline']:\n",
    "        # Plot them\n",
    "        figdim = np.array([5,4])\n",
    "        fig, axes = plt.subplots(len(uncertain_params)-1, len(uncertain_params)-1, figsize=figdim*6)\n",
    "\n",
    "        for pair_i, (param_i, param_j) in enumerate(param_pairs):\n",
    "            results_pair = all_results[condition_key][pair_i]\n",
    "\n",
    "            if condition_key == 'uncertain-baseline':\n",
    "                cmap = 'PuOr_r'\n",
    "                # Get extreme value of result for colorbar limits\n",
    "                extreme = max([np.abs(np.nanmin(results_pair)), np.nanmax(results_pair)])\n",
    "                norm = colors.TwoSlopeNorm(vmin=-extreme, vcenter=0, vmax=extreme)\n",
    "            else:\n",
    "                cmap = 'viridis'\n",
    "                norm = colors.Normalize(vmin=np.nanmin(results_pair), vmax=np.nanmax(results_pair))\n",
    "\n",
    "            if param_i in [0, 2, 4]:\n",
    "                origin = 'upper'\n",
    "            else:\n",
    "                origin = 'lower'\n",
    "\n",
    "            if param_j  in [0, 2, 4]:\n",
    "                results_pair = np.fliplr(results_pair.copy())\n",
    "                x_axis = np.flip(param_cntrs[param_j])\n",
    "            else:\n",
    "                x_axis = param_cntrs[param_j]\n",
    "\n",
    "            im = axes[param_i,param_j-1].imshow(results_pair, origin=origin, norm=norm, cmap=cmap)\n",
    "            if param_j == len(uncertain_params) - 1:\n",
    "                # label = r'$\\Delta P(S \\geq S^*)$' if condition_key == 'uncertain-baseline' else r'$P(S \\geq S^*)$'\n",
    "                label = r'$\\Delta$ contribution to $\\omega$' if condition_key == 'uncertain-baseline' else r'contribution to $\\omega$'\n",
    "                cbar = fig.colorbar(im, shrink=0.75, label=label)\n",
    "            else:\n",
    "                cbar = fig.colorbar(im, shrink=0.75)\n",
    "            cbar.ax.tick_params(labelsize=plt.rcParams['axes.labelsize'] * 0.5)\n",
    "            tick_spacing = 1 if num_param_bins <= 5 else 2\n",
    "            axes[param_i,param_j-1].set_xlabel(param_labels[param_j], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "            axes[param_i,param_j-1].set_xticks(np.arange(num_param_bins)[::tick_spacing],\n",
    "                                               np.round(x_axis, 2)[::tick_spacing],\n",
    "                                               size=plt.rcParams['axes.labelsize']*0.25)\n",
    "            axes[param_i,param_j-1].set_ylabel(param_labels[param_i], fontsize=plt.rcParams['axes.titlesize']*1.75)\n",
    "            axes[param_i,param_j-1].set_yticks(np.arange(num_param_bins)[::tick_spacing], np.round(param_cntrs[param_i], 2)[::tick_spacing])\n",
    "            axes[param_i,param_j-1].tick_params(axis='both', labelsize=plt.rcParams['axes.labelsize'] * 0.75)\n",
    "\n",
    "        if condition_key == 'uncertain-baseline':\n",
    "            fig.savefig(fig_prefix + f'uncertainty_pairs_{condition_key}_{np.round(q_vec[q_i],2)}.png', bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7817323-2734-4557-9ad3-88839a9acab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(all_results['uncertain-baseline'].shape[0]):\n",
    "    param_i, param_j = param_pairs[i]\n",
    "    print(uncertain_params[param_i], uncertain_params[param_j])\n",
    "    print(np.sum(all_results['uncertain-baseline'][i].ravel()))\n",
    "    plt.hist(all_results['uncertain-baseline'][i].ravel());\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d74239-e437-454a-af0e-317d4eacaf5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Single parameter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27c71c-d8af-4b26-aa8b-7eba0bfadf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'demographic_index']\n",
    "\n",
    "# Specify resource constraint and decision parameters\n",
    "C = 9 * ncell_tot\n",
    "n, l = (55_000, 22_000)\n",
    "\n",
    "# Find their exact values in the decision parameter vectors\n",
    "C = C_vec[np.argmin(np.abs(C_vec - C))]\n",
    "n = ncell_vec[np.argmin(np.abs(ncell_vec - n))]\n",
    "l = slice_left_all[np.argmin(np.abs(slice_left_all - l))]\n",
    "\n",
    "# Get the slice of range-wide stability at the above values\n",
    "decision_filt = (x_all[:, 0] == C) & (x_all[:, 1] == n) & (x_all[:, 2] == l)\n",
    "S_slice = meta_metric_all[decision_filt]\n",
    "x_uncertain = x_all[decision_filt, 3:]\n",
    "\n",
    "for param_i in range(x_uncertain.shape[-1]):\n",
    "    # Bin the parameter along its sampled range\n",
    "    num_param_bins = 10\n",
    "    param_low = x_uncertain[:, param_i].min()\n",
    "    param_high = x_uncertain[:, param_i].max()\n",
    "    param_edges, step = np.linspace(param_low, param_high, num_param_bins, retstep=True)\n",
    "    param_cntrs = param_edges[:-1] + step/2\n",
    "    \n",
    "    # Get the means and upper/lower percentiles of S within each bin\n",
    "    S_means = np.empty_like(param_cntrs)\n",
    "    errors = np.empty((2, len(param_cntrs)))\n",
    "    for cntr_i, edge in enumerate(param_edges[:-1]):\n",
    "        _filt = (x_uncertain[:, param_i] > edge) & (x_uncertain[:, param_i] <= edge + step)\n",
    "        S_means[cntr_i] = np.mean(S_slice[_filt])\n",
    "        errors[0, cntr_i] = np.percentile(S_slice[_filt], 25)\n",
    "        errors[1, cntr_i] = np.percentile(S_slice[_filt], 75)\n",
    "    \n",
    "    # Plot\n",
    "    plt.errorbar(param_cntrs, S_means, yerr=errors, marker='o', label=uncertain_params[param_i])\n",
    "    plt.ylim(S_slice.min(), S_slice.max())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
