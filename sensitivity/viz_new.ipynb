{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abf5dc-f407-4182-a9e5-f31979ab41a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import pickle\n",
    "import signac as sg\n",
    "from scipy.special import gamma\n",
    "import copy as copy\n",
    "import scipy\n",
    "from global_functions import adjustmaps\n",
    "import h5py\n",
    "from scipy.interpolate import make_lsq_spline\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# Define/load things non-specific to a given set of results\n",
    "metric = 'lambda_s'\n",
    "Aeff = 7.29\n",
    "t_final = 300\n",
    "ncell_tot = 87_993\n",
    "c = 1.42\n",
    "with sg.H5Store('shared_data.h5').open(mode='r') as sd:\n",
    "    b_vec = np.array(sd['b_vec'])\n",
    "tau_vec = b_vec * gamma(1+1/c)\n",
    "tauc_methods = [\"flat\"]\n",
    "results_pre = 'gte_thresh' \n",
    "\n",
    "# Update global plotting parameters\n",
    "rc('axes', labelsize=21)  # Font size for x and y labels\n",
    "rc('axes', titlesize=16)\n",
    "rc('xtick', labelsize=19)  # Font size for x-axis tick labels\n",
    "rc('ytick', labelsize=19)  # Font size for y-axis tick labels\n",
    "rc('lines', markersize=15)  \n",
    "rc('lines', linewidth=5.5)\n",
    "rc('legend', fontsize=19)\n",
    "rc('font', family='sans-serif')\n",
    "rc('font', serif=['Computer Modern Sans Serif'] + plt.rcParams['font.serif'])\n",
    "rc('font', weight='light')\n",
    "histlw = 5.5\n",
    "cbar_lpad = 30\n",
    "dpi = 50\n",
    "# dpi = 200\n",
    "\n",
    "# Function to read in things specific to given results as global variables\n",
    "def set_globals(results_pre):\n",
    "    if metric == 'lambda_s':\n",
    "        globals()['metric_lab'] = r'$S$'\n",
    "        globals()['rob_metric_lab'] = r'$S^*$'\n",
    "        globals()['mean_metric_lab'] = r'$\\bar{\\lambda}(\\tau)$'\n",
    "    if metric == 'P_s':\n",
    "        globals()['metric_lab'] = r'$S_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\S_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<P_s>$'\n",
    "    if metric == 's':\n",
    "        globals()['metric_lab'] = r'$s_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\s_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<s>$'\n",
    "    globals()['fn_prefix'] = f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    globals()['fig_prefix'] = f\"{results_pre}/figs/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    # globals()['fig_prefix'] = os.path.join('/','Volumes', 'Macintosh HD', 'Users', 'patrick', \n",
    "    #                                        'Google Drive', 'My Drive', 'Research', 'Regan', 'Figs/')\n",
    "\n",
    "    # Load things saved specific to these results\n",
    "    globals()['metric_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/metric_all.npy\")\n",
    "    globals()['tau_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/tau_all.npy\")\n",
    "    globals()['C_vec'] = np.load(fn_prefix + \"C_vec.npy\")\n",
    "    globals()['C_i_vec'] = np.arange(len(C_vec))[::2]\n",
    "    globals()['ncell_vec'] = np.load(fn_prefix + \"ncell_vec.npy\")\n",
    "    globals()['slice_left_all'] = np.load(fn_prefix + \"slice_left_all.npy\")\n",
    "    eps_axes = {}\n",
    "    with h5py.File(fn_prefix + \"eps_axes.h5\", \"r\") as handle:\n",
    "        for key in handle.keys():\n",
    "            eps_axes.update({key: handle[key][()]})\n",
    "    globals()['eps_axes'] = eps_axes\n",
    "\n",
    "# Read in maps and convert fdm to tau, used by multiple plots below\n",
    "ul_coord = [1500, 2800]\n",
    "lr_coord = [2723, 3905]\n",
    "usecols = np.arange(ul_coord[0],lr_coord[0])\n",
    "sdmfn = \"../shared_maps/SDM_1995.asc\"\n",
    "sdm = np.loadtxt(sdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "fdmfn = '../shared_maps/FDE_current_allregions.asc'\n",
    "fdm = np.loadtxt(fdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "sdm, fdm = adjustmaps([sdm, fdm])\n",
    "delta_t = 30\n",
    "b_raster = delta_t / np.power(-np.log(1-fdm), 1/c)\n",
    "tau_raster = b_raster * gamma(1+1/c)\n",
    "maps_filt = (sdm > 0) & (fdm > 0)\n",
    "tau_flat = tau_raster[maps_filt] \n",
    "mapindices = np.argwhere(maps_filt)\n",
    "tau_argsort = np.argsort(tau_flat)\n",
    "tau_sorted = tau_flat[tau_argsort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245b13c-6547-4195-82eb-5312a29bf5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_globals(results_pre)\n",
    "x_all = np.load(fn_prefix + '/x_all.npy')\n",
    "meta_metric_all = np.load(fn_prefix + '/meta_metric_all.npy')\n",
    "meta_metric_all = meta_metric_all[:,0]\n",
    "\n",
    "meta_metric_nochange = float(np.load(fn_prefix + 'meta_metric_nochange.npy'))\n",
    "\n",
    "# Create a filter for the baseline scenario\n",
    "zero_eps_mask = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "\n",
    "maxrob = np.load(fn_prefix + \"maxrob.npy\")\n",
    "argmaxrob = np.load(fn_prefix + \"argmaxrob.npy\")\n",
    "rob_thresh_vec = np.load(fn_prefix + \"rob_thresh_vec.npy\")\n",
    "rob_all = np.load(fn_prefix + \"rob_all.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b7c12-24ff-470d-9568-a03959eb8061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Looking at raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218fabf-9e87-405a-8102-798bc790c53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# q_vec = np.arange(0.1, 0.9, 0.1)\n",
    "q_vec = np.arange(0.0, 0.85, 0.05)\n",
    "\n",
    "# results = {}\n",
    "keys = ['delta_range', 'delta_median', 'median', 'range', 'delta_n', 'delta_l', 'n', 'l',\n",
    "        'delta_taul', 'delta_tauh']\n",
    "results = {key: np.full((C_vec.size, q_vec.size), np.nan) for key in keys}\n",
    "results['n_baseline'] = np.full(C_vec.size, np.nan)\n",
    "results['l_baseline'] = np.full(C_vec.size, np.nan)\n",
    "\n",
    "for C_i, C in enumerate(C_vec):\n",
    "    for q_i, q in enumerate(q_vec):\n",
    "        if q_i == 0:\n",
    "            # First get optimal tau slice stats from optimal decisions under baseline\n",
    "            zeroeps_filt = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "            _filt = zeroeps_filt & (x_all[:,0] == C)\n",
    "            argmax = np.nanargmax(meta_metric_all[_filt])\n",
    "            S_opt_baseline = meta_metric_all[_filt][argmax]\n",
    "            n_opt_baseline, sl_opt_baseline = x_all[_filt,:][argmax][1:3].astype(int)\n",
    "            tau_slice_baseline = tau_sorted[sl_opt_baseline:sl_opt_baseline+n_opt_baseline]\n",
    "            range_baseline = tau_slice_baseline.max() - tau_slice_baseline.min()\n",
    "            median_baseline = np.median(tau_slice_baseline)\n",
    "            \n",
    "            results['n_baseline'][C_i] = n_opt_baseline\n",
    "            results['l_baseline'][C_i] = sl_opt_baseline\n",
    "\n",
    "        # Now get the same statistics for (1-q) * optimal S baseline\n",
    "        Sstar_rob_i = np.argmin(np.abs(rob_thresh_vec - ((1 - q) * S_opt_baseline)) )\n",
    "        n_opt_rob = ncell_vec[int(argmaxrob[Sstar_rob_i, C_i][0])]\n",
    "        sl_opt_rob = slice_left_all[int(argmaxrob[Sstar_rob_i, C_i][1])]\n",
    "        tau_slice_rob = tau_sorted[sl_opt_rob:sl_opt_rob+n_opt_rob]\n",
    "        range_rob = tau_slice_rob.max() - tau_slice_rob.min()\n",
    "        median_rob = np.median(tau_slice_rob)\n",
    "\n",
    "        results['delta_median'][C_i, q_i] = median_rob - median_baseline\n",
    "        results['delta_range'][C_i, q_i] = range_rob - range_baseline\n",
    "        results['median'][C_i, q_i] = median_rob\n",
    "        results['range'][C_i, q_i] = range_rob\n",
    "        results['delta_n'][C_i, q_i] = n_opt_rob - n_opt_baseline\n",
    "        results['delta_l'][C_i, q_i] = sl_opt_rob - sl_opt_baseline\n",
    "        results['n'][C_i, q_i] = n_opt_rob\n",
    "        results['l'][C_i, q_i] = sl_opt_rob\n",
    "        results['delta_taul'][C_i][q_i] = tau_sorted[sl_opt_rob] - tau_sorted[sl_opt_baseline]\n",
    "        results['delta_tauh'][C_i][q_i] = tau_sorted[sl_opt_rob+n_opt_rob] - tau_sorted[sl_opt_baseline+n_opt_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafc243-3af9-4566-a678-79c6822d7977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results['delta_l+delta_n'] = (results['delta_l'] + results['delta_n']) #/ np.abs(results['delta_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860096cd-3e87-4f83-8f8d-4f4ff1808638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reference indices for per population tau\n",
    "tau_indices = np.arange(tau_sorted.size)\n",
    "\n",
    "# Set the R value we're plotting\n",
    "C = 9\n",
    "assert C in C_vec/ncell_tot\n",
    "C_i = (C_vec/ncell_tot == C).argmax()\n",
    "# print(f\"at baseline for C={C_vec[C_i]} \\n optimal n,l = {results['n_baseline'][C_i], results['l_baseline'][C_i]} \\n\")\n",
    "\n",
    "for q in [0.0, 0.45, 0.8]:\n",
    "# for q in [0.5]:\n",
    "    # Set the S^* value we're plotting\n",
    "    assert np.any(np.isclose(q_vec, q))\n",
    "    q_i = np.isclose(q_vec, q).argmax()\n",
    "\n",
    "    # Get the optimal decision at this {S^*, R} combination\n",
    "    n_opt = results['n'][C_i, q_i]\n",
    "    l_opt = results['l'][C_i, q_i]\n",
    "    # print(f'at q={q}\\n optimal n,l = {n_opt, l_opt} \\n')\n",
    "\n",
    "    # Define results vector\n",
    "    results_vector = np.full(tau_sorted.size, np.nan)\n",
    "    # where each population is given a number to indicate optimality under:\n",
    "    #   1 -> baseline condiitons only\n",
    "    #   2 -> baseline and uncertain conditions (risk aversion)\n",
    "    #   3 -> uncertain conditions only\n",
    "    #   0 -> everything else\n",
    "    \n",
    "    # Create relevant masks\n",
    "    baseline_mask = (tau_indices > results['l_baseline'][C_i]) & (tau_indices < results['l_baseline'][C_i] + results['n_baseline'][C_i])\n",
    "    uncertain_mask = (tau_indices > l_opt) & (tau_indices < l_opt + n_opt)\n",
    "    baseline_only_mask = baseline_mask & (~uncertain_mask)\n",
    "    uncertain_only_mask = uncertain_mask & (~baseline_mask)\n",
    "    both_mask = baseline_mask & uncertain_mask\n",
    "    neither_mask = ~(baseline_mask | uncertain_mask)\n",
    "    \n",
    "    # Use masks to assign values to each population\n",
    "    results_vector[neither_mask] = 0\n",
    "    results_vector[baseline_only_mask] = 1\n",
    "    results_vector[both_mask] = 2\n",
    "    results_vector[uncertain_only_mask] = 3\n",
    "    \n",
    "    # Define colormaping for categories\n",
    "    custom_colors = ['lightgrey', 'coral', 'orchid', 'blueviolet']\n",
    "    labels = ['neither', 'baseline only', 'both', 'uncertain only']\n",
    "    cmap = colors.ListedColormap(custom_colors)\n",
    "    vmin = 0; vmax = len(custom_colors) - 1\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ### TAU DISTRIBUTION VIZ ###\n",
    "    \n",
    "    stack_data = [tau_sorted[results_vector == i] for i in range(len(custom_colors))]\n",
    "    \n",
    "    bins = np.linspace(min(tau_flat), 50, 80)\n",
    "    \n",
    "    # Plot the stacked histogram\n",
    "    fig, ax = plt.subplots(figsize=np.array([8,5])*0.75)\n",
    "    ax.hist(\n",
    "        stack_data,\n",
    "        bins=bins,\n",
    "        stacked=True,\n",
    "        color=custom_colors,\n",
    "        label=[labels[i] for i in range(len(custom_colors))]\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r'$\\tau$')\n",
    "    ax.set_ylabel(r'baseline $\\tau$ frequency')\n",
    "    ax.set_title(f'$q=${q}')\n",
    "    ax.legend()\n",
    "    \n",
    "    ### GEOGRAPHICAL MAP ###\n",
    "\n",
    "    mapi_sorted = mapindices[tau_argsort].T\n",
    "\n",
    "    colored_data = np.ones(maps_filt.shape + (4,)) * np.nan #colors in rgba\n",
    "    colored_data[mapi_sorted[0], mapi_sorted[1]] = cmap(norm(results_vector))\n",
    "    # Color background\n",
    "    colored_data[maps_filt == False] = colors.to_rgba('black', alpha=0.3)\n",
    "    # Crop out border where all nans\n",
    "    nonzero_indices = np.nonzero(maps_filt)\n",
    "    row_min, row_max = nonzero_indices[0].min(), nonzero_indices[0].max()\n",
    "    col_min, col_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    colored_data = colored_data[row_min:row_max + 1, col_min:col_max + 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=np.array([10,10])*0.7)\n",
    "    im = ax.imshow(colored_data)#, aspect='auto')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1604aa-df02-43dc-8356-7cddff054d6a",
   "metadata": {},
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2699d-7027-446d-95c3-aeccc17b4574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [r'$\\Delta(\\tau~\\text{range})$', r'$\\Delta(\\tau~\\text{median})$', \n",
    "          r'$\\tau~\\text{range}$', r'$\\tau~\\text{median}$',\n",
    "          r'$\\Delta l + \\Delta n$',\n",
    "          r'$\\Delta n$', r'$\\Delta l$', r'$n$', r'$l$',\n",
    "          r'$\\Delta \\tau_l$', r'$\\Delta \\tau_h$']\n",
    "# cmap = copy.copy(cm.RdPu_r)\n",
    "# cmap = copy.copy(cm.Wistia)\n",
    "cmap = copy.copy(cm.cool)\n",
    "vmin = C_vec.min() if (len(C_vec) > 1) else 0\n",
    "norm = colors.Normalize(vmin=vmin/ncell_tot, vmax=C_vec.max()/ncell_tot)\n",
    "\n",
    "# Restrict the range of plotting to a desired q value\n",
    "q_lim = 0.85\n",
    "q_mask = q_vec <= q_lim \n",
    "\n",
    "for key_i, key in enumerate(['delta_range', 'delta_median', 'range', 'median',\n",
    "                             'delta_l+delta_n', 'delta_n', 'delta_l', 'n', 'l',\n",
    "                             'delta_taul', 'delta_tauh']):\n",
    "    fig, ax = plt.subplots(figsize=(9,5))\n",
    "    \n",
    "    for C_i, C in enumerate(C_vec):\n",
    "        # if (C_i) % 2 == 0: continue\n",
    "        # if C_i < 3: continue\n",
    "        # if C_i > 3: continue\n",
    "        # if C_i != len(C_vec) - 1: continue\n",
    "        # if (C/ncell_tot) < 9: continue\n",
    "        # if (C/ncell_tot) != 5: continue\n",
    "        color = cmap(norm(C/ncell_tot))\n",
    "        ax.scatter(q_vec[q_mask]*100, results[key][C_i, q_mask], marker='o', c=color, alpha=0.8)\n",
    "        \n",
    "    # ax.set_xlabel(r'$q$; $S^*=(1-q)\\text{max}(S_{baseline})$')\n",
    "    ax.set_xlabel(r'% of $\\text{max}(S_{baseline})$ sacrificed')\n",
    "    ax.set_ylabel(labels[key_i])\n",
    "    \n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array(C_vec)\n",
    "    cbar = fig.colorbar(sm, label=r'$R~/~n_{tot}$', ax=ax)\n",
    "    \n",
    "    if key not in ['range', 'median', 'n', 'l']:\n",
    "        ax.axhline(0, ls='--', lw=1, c='k')\n",
    "    # ax.set_xlim(0.025,0.525)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d895f-f1ad-4aef-9fb8-6000b1db653f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interpolate $\\omega$ to get optimal $\\{n,l\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc6f47-ea91-46fa-866d-63336decc5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class to rescale inputs and outputs to [0,1] for numerical stability\n",
    "# Also store descalers to interpret interpolated values\n",
    "class Rescaler:\n",
    "    def __init__(self, mins, maxes):\n",
    "        \"\"\"\n",
    "        mins: vector of minima\n",
    "        maxes: vector of maxima\n",
    "        \"\"\"\n",
    "        self.mins = mins\n",
    "        self.maxes = maxes\n",
    "\n",
    "    def rescale(self, x):\n",
    "        return (x - self.mins) / (self.maxes - self.mins)\n",
    "    \n",
    "    def descale(self, x):\n",
    "        return (x * (self.maxes - self.mins)) + self.mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84ee07-37f2-4975-9449-1ab278b3fc4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3rd attempt\n",
    "Create separate interpolators for robustness(n,l) at each S^* values. Could probably get away with using RectBivariateSpline if we assign nan robustness values to zero (or something). Use RegularGridInterpolator for baseline S(n, l) bc we dont need smoothing there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ebdcd-9878-4751-a7be-032dbe8a5d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# Rescale inputs (i.e. decision parameters), these will be used for all interpolations\n",
    "n_rescaler = Rescaler(ncell_vec.min(), ncell_vec.max())\n",
    "n_rescaled = n_rescaler.rescale(ncell_vec)\n",
    "l_rescaler = Rescaler(slice_left_all.min(), slice_left_all.max())\n",
    "l_rescaled = l_rescaler.rescale(slice_left_all)\n",
    "\n",
    "# Pick an Sstar value for an example\n",
    "Sstar_i = 70\n",
    "Sstar = rob_thresh_vec[Sstar_i]\n",
    "\n",
    "# Filter robustness samples for this Sstar and C value\n",
    "rob_all_filtered = rob_all[Sstar_i, C_i, ...].copy()\n",
    "\n",
    "# Insert zeros for nan values (i.e. invalid parameter combinations)\n",
    "nan_filt = np.isnan(rob_all_filtered)\n",
    "rob_all_filtered[nan_filt] = 0.0\n",
    "\n",
    "# Rescale robustness values\n",
    "rob_rescaler = Rescaler(rob_all_filtered.min(), rob_all_filtered.max())\n",
    "rob_rescaled = rob_rescaler.rescale(rob_all_filtered)\n",
    "\n",
    "# Interpolate robustness with smoothing set by s parameter\n",
    "k = 1\n",
    "s = 7\n",
    "interp = RectBivariateSpline(n_rescaled, l_rescaled, rob_rescaled, s=s, kx=k, ky=k)\n",
    "\n",
    "plt.imshow(rob_all_filtered, origin='lower')\n",
    "n_opt_i, l_opt_i = np.unravel_index(rob_all_filtered.argmax(), rob_all_filtered.shape)\n",
    "plt.scatter(l_opt_i, n_opt_i, marker='x', s=120, linewidths=2.2, c='r')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "n_test = np.linspace(0, 1, 500)\n",
    "l_test = np.linspace(0, 1, int((slice_left_all.size / ncell_vec.size) * n_test.size))\n",
    "interpolated_rob = interp(n_test, l_test)\n",
    "plt.imshow(interpolated_rob, origin='lower')\n",
    "n_opt_i, l_opt_i = np.unravel_index(interpolated_rob.argmax(), interpolated_rob.shape)\n",
    "plt.scatter(l_opt_i, n_opt_i, marker='x', s=120, linewidths=2.2, c='r')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5984176-1eb5-41e5-a2f9-48bfd4239470",
   "metadata": {},
   "source": [
    "innermost is y (l), outermost is x (n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ecd49-fb4b-4011-bf16-92f6c1675a11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iterating over all S^*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1f606-b0c9-437c-b687-11c13992b530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "NN_ROB = 150\n",
    "NN_S = 100\n",
    "# SMOOTHING = 0.15\n",
    "SMOOTHING = 0.01\n",
    "\n",
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# First, filter robustness values for this R value\n",
    "rob_all_filtered = rob_all[:, 0, ...]\n",
    "\n",
    "# Unravel the filtered robustness values into y_obs\n",
    "y_obs = rob_all_filtered.flatten()\n",
    "\n",
    "# Place the corresponding decision parameter values into x_obs\n",
    "indices = np.unravel_index(np.arange(y_obs.size), rob_all_filtered.shape)\n",
    "x_obs = np.full((y_obs.size, len(rob_all_filtered.shape)), np.nan)\n",
    "x_obs[:, 0] = rob_thresh_vec[indices[0]]\n",
    "x_obs[:, 1] = ncell_vec[indices[1]]\n",
    "x_obs[:, 2] = slice_left_all[indices[2]]\n",
    "\n",
    "# Filter out any invalid param combinations from both x and y\n",
    "nan_filt = np.isnan(y_obs)\n",
    "# y_obs = y_obs[~nan_filt]\n",
    "# x_obs = x_obs[~nan_filt, :]\n",
    "'''Instead try putting zeros in y_obs for nan'''\n",
    "y_obs[nan_filt] = 0.0\n",
    "\n",
    "# Rescale the y values (i.e. the robustness values)\n",
    "y_rescaler = Rescaler(y_obs.min(axis=0), y_obs.max(axis=0))\n",
    "y_obs = y_rescaler.rescale(y_obs) \n",
    "    \n",
    "# Rescale the x values (i.e. the decision parameters)\n",
    "x_rescaler = Rescaler(x_obs.min(axis=0), x_obs.max(axis=0))\n",
    "x_obs = x_rescaler.rescale(x_obs)\n",
    "\n",
    "# Create interpolator for robustness(S^*, n, l) given R\n",
    "interp = RBFInterpolator(x_obs, y_obs, neighbors=NN_ROB, smoothing=SMOOTHING)\n",
    "\n",
    "# Get parameters and S values under baseline condintions\n",
    "C_mask = (x_all[:, 0] == (C*ncell_tot))\n",
    "baseline_mask = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "x_obs_baseline = x_all[C_mask & baseline_mask, 1:3]\n",
    "y_obs_baseline = meta_metric_all[C_mask & baseline_mask]\n",
    "\n",
    "# Rescale inputs and outputs\n",
    "x_rescaler_baseline = Rescaler(x_obs_baseline.min(axis=0), x_obs_baseline.max(axis=0))\n",
    "x_obs_baseline = x_rescaler_baseline.rescale(x_obs_baseline)\n",
    "y_rescaler_baseline = Rescaler(y_obs_baseline.min(axis=0), y_obs_baseline.max(axis=0))\n",
    "y_obs_baseline = y_rescaler_baseline.rescale(y_obs_baseline) \n",
    "\n",
    "# Interpolate S(n, l) given R under baseline conditions for reference during optimization\n",
    "interp_baseline = RBFInterpolator(x_obs_baseline, y_obs_baseline, neighbors=NN_S, smoothing=0.0)\n",
    "\n",
    "# First optimize decision under baseline conditions\n",
    "def objective_baseline(decision_params):\n",
    "    S = interp_baseline([decision_params])\n",
    "    return -S\n",
    "\n",
    "# Use optimal decision from exisiting samples as starting point\n",
    "zeroeps_filt = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "_filt = zeroeps_filt & (x_all[:,0] == C*ncell_tot)\n",
    "argmax = np.nanargmax(meta_metric_all[_filt])\n",
    "n0, l0 = x_all[_filt,:][argmax][1:3].astype(int)\n",
    "n0, l0 = x_rescaler_baseline.rescale([n0, l0])\n",
    "\n",
    "# Optimize using scipy\n",
    "x0 = np.array([n0, l0])\n",
    "bounds = ((0, 1), (0, 1)) # Remeber, we rescaled the training data\n",
    "cons = [{'type': 'ineq', 'fun': lambda x:  1 - x[1] - x[0]}] # Constrain l < (n_tot - n)\n",
    "res = minimize(objective_baseline, x0, method='COBYLA', bounds=bounds, constraints=cons)\n",
    "n_opt_baseline, l_opt_baseline = x_rescaler_baseline.descale(res.x).astype(int)\n",
    "S_opt_baseline = y_rescaler_baseline.descale(-res.fun)\n",
    "'''Fudge factor'''\n",
    "S_opt_baseline *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7ead3-da7c-48c2-98e0-05175cda0122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_RESTARTS = 5\n",
    "\n",
    "# Define objective function (i.e. robustness) to be optimized\n",
    "def objective(decision_params, *args):\n",
    "    '''This should get descaled before proceeding'''\n",
    "    Sstar = args[0] \n",
    "    # Sstar_descaled = x_rescaler.descale([Sstar, decision_params[0], decision_params[1]])[0]\n",
    "    \n",
    "    # S_baseline = interp_baseline([decision_params])\n",
    "    # S_baseline = y_rescaler_baseline.descale(S_baseline) \n",
    "    # if S_baseline < Sstar_descaled:\n",
    "    #     # Only consider robustness to be nonzero if S^* met under baseline\n",
    "    #     robustness = 0\n",
    "    # else:\n",
    "    if True:\n",
    "        # Take robustness value from interpolation\n",
    "        x = np.full(len(decision_params)+1, np.nan)\n",
    "        x[0] = Sstar\n",
    "        x[1:] = decision_params\n",
    "        try:\n",
    "            robustness = interp([x])\n",
    "        except:\n",
    "            # print(x)\n",
    "            # import sys; sys.exit()\n",
    "            robustness = 0\n",
    "        # print(robustness)\n",
    "    \n",
    "    return -robustness # Negate bc using minimization algorithm\n",
    "\n",
    "# Now step through S^* values and find decisions that optimize robustness\n",
    "n_opt_interp = np.full(rob_thresh_vec.size, np.nan)\n",
    "l_opt_interp = np.full(rob_thresh_vec.size, np.nan)\n",
    "for Sstar_i, Sstar in enumerate(rob_thresh_vec):\n",
    "    # Use optimal decisions from exisiting samples as starting points\n",
    "    argsort = np.argsort(rob_all_filtered[Sstar_i, :], axis=None)\n",
    "    nan_filt = np.isnan(rob_all_filtered[Sstar_i, :].ravel()[argsort])\n",
    "    argsort = argsort[~nan_filt]\n",
    "    \n",
    "    n_opt_samples = []\n",
    "    l_opt_samples = []\n",
    "    \n",
    "    for x0_position in argsort[-NUM_RESTARTS:]:\n",
    "        n0_i, l0_i = np.unravel_index(x0_position, rob_all_filtered.shape[1:])\n",
    "        n0, l0 = (ncell_vec[n0_i], slice_left_all[l0_i])\n",
    "\n",
    "        # Rescale to interpolation scale\n",
    "        Sstar, n0, l0 = x_rescaler.rescale([Sstar, n0, l0])\n",
    "\n",
    "        # Use an optimizer that can handle some noise in the objective\n",
    "        x0 = np.array([n0, l0])\n",
    "        # bounds = ((0, 1), (0, 1)) # Remeber, we rescaled the training data\n",
    "        # cons = [{'type': 'ineq', 'fun': lambda x:  1 - x[1] - x[0]}] # Constrain l < (n_tot - n)\n",
    "        # res = minimize(objective, x0, args=(Sstar), method='COBYLA', bounds=bounds, constraints=cons)\n",
    "        cons = [\n",
    "            {'type': 'ineq', 'fun': lambda x: x[0]},          # x[0] >= 0\n",
    "            {'type': 'ineq', 'fun': lambda x: 1 - x[0]},      # x[0] <= 1\n",
    "            {'type': 'ineq', 'fun': lambda x: x[1]},          # x[1] >= 0\n",
    "            {'type': 'ineq', 'fun': lambda x: 1 - x[1]},      # x[1] <= 1\n",
    "            {'type': 'ineq', 'fun': lambda x: 1 - x[0] - x[1]}  # your original constraint: l < (1 - n)\n",
    "        ]\n",
    "        res = minimize(objective, x0, args=(Sstar,), method='COBYLA', constraints=cons)\n",
    "\n",
    "        _, n_opt, l_opt = x_rescaler.descale([Sstar, res.x[0], res.x[1]])\n",
    "        n_opt_samples.append(n_opt)\n",
    "        l_opt_samples.append(l_opt)\n",
    "        \n",
    "    # Take the mean over multiple optimization runs\n",
    "    n_opt_interp[Sstar_i] = np.mean(n_opt_samples)\n",
    "    l_opt_interp[Sstar_i] = np.mean(l_opt_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401a304-540b-4603-a2de-7d4f6b6b8503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# q_vec = np.arange(0.0, 0.85, 0.05)\n",
    "q_vec = np.arange(0.0, 1.0, 0.05)\n",
    "\n",
    "delta_taul_interp = np.full(q_vec.size, np.nan)\n",
    "delta_tauh_interp = np.full(q_vec.size, np.nan)\n",
    "\n",
    "for q_i, q in enumerate(q_vec):\n",
    "    # Now get the optimal decisions for (1-q) * optimal S baseline\n",
    "    Sstar_i = np.argmin(np.abs(rob_thresh_vec - ((1 - q) * S_opt_baseline)) )\n",
    "    n_opt_rob = int(n_opt_interp[Sstar_i])\n",
    "    l_opt_rob = int(l_opt_interp[Sstar_i])\n",
    "    \n",
    "    # Replace this q value with the closest one we have available\n",
    "    Sstar = rob_thresh_vec[Sstar_i]\n",
    "    q_vec[q_i] = 1 - (Sstar / S_opt_baseline)\n",
    "        \n",
    "    delta_taul_interp[q_i] = tau_sorted[l_opt_rob] - tau_sorted[l_opt_baseline]\n",
    "    delta_tauh_interp[q_i] = tau_sorted[l_opt_rob+n_opt_rob] - tau_sorted[l_opt_baseline+n_opt_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf91df2-5a0e-415a-a4b8-8e3e5f4b0ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# shapes = ['x', 'o', 's']\n",
    "ls = ['-', '--', '-.', ':']\n",
    "for i, q in enumerate([0.0, 0.2, 0.5]):\n",
    "    if q == 0:\n",
    "        label = r'baseline $S_\\text{opt}$'\n",
    "        ax.axvline(S_opt_baseline, ls=ls[i], c='limegreen', label=label, lw=3)\n",
    "        plt.scatter(S_opt_baseline, n_opt_baseline, marker='o', c='limegreen', label=r'baseline $n_\\text{opt}$')\n",
    "        plt.scatter(S_opt_baseline, l_opt_baseline, marker='x', c='limegreen', label=r'baseline $l_\\text{opt}$')\n",
    "    else:\n",
    "        # Set the S^* value we're plotting\n",
    "        q_i = np.argmin(np.abs(q_vec - q))\n",
    "        q = q_vec[q_i]\n",
    "        # Get the optimal decision at this {S^*, R} combination\n",
    "        Sstar_i = np.argmin(np.abs(rob_thresh_vec - ((1 - q) * S_opt_baseline)) )\n",
    "        Sstar = rob_thresh_vec[Sstar_i]\n",
    "        label = rf'{np.round(100*q, 1)}% sacrificed'\n",
    "        ax.axvline(Sstar, ls=ls[i], c='k', label=label, lw=3)\n",
    "\n",
    "spacing = 5\n",
    "plt_filt = rob_thresh_vec <= S_opt_baseline\n",
    "# plt_filt = rob_thresh_vec <= 1\n",
    "ax.scatter(rob_thresh_vec[plt_filt][::spacing], n_opt_interp[plt_filt][::spacing], label=r'robust $n_\\text{opt}$', c='k', marker='o')\n",
    "ax.scatter(rob_thresh_vec[plt_filt][::spacing], l_opt_interp[plt_filt][::spacing], label=r'robust $l_\\text{opt}$', c='k', marker='x')\n",
    "closest_i_baseline = np.argmin(np.abs(rob_thresh_vec - S_opt_baseline))\n",
    "ax.scatter(rob_thresh_vec[closest_i_baseline], n_opt_interp[closest_i_baseline], c='k', marker='o')\n",
    "ax.scatter(rob_thresh_vec[closest_i_baseline], l_opt_interp[closest_i_baseline], c='k', marker='x')\n",
    "        \n",
    "# ax.legend(fontsize='12')\n",
    "fig_legend = plt.figure(figsize=(1, 0.5)) # Adjust size as needed for your legend\n",
    "ax_legend = fig_legend.add_subplot(111)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax_legend.legend(handles, labels, loc='center', frameon=False) # Customize loc and frameon as desired\n",
    "ax_legend.axis('off') # Hide the axes\n",
    "\n",
    "ax.set_xlabel(r'$S^*$')\n",
    "# ax.set_xlim(meta_metric_nochange, 1.0);\n",
    "ax.set_xlim(meta_metric_nochange, S_opt_baseline*1.025);\n",
    "\n",
    "fig.savefig(fig_prefix + '/nandloptvsSstar.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b9bfe-df47-4514-9bef-b90f5ef4dfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restrict the range of plotting to a desired q value\n",
    "q_lim = 0.85\n",
    "# q_lim = 1.\n",
    "q_mask = q_vec <= q_lim \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "ax.scatter(q_vec[q_mask]*100, delta_taul_interp[q_mask], label=r'lowest $\\tau$')\n",
    "ax.axhline(0, ls='--', c='k', lw=1)\n",
    "\n",
    "ax.scatter(q_vec[q_mask]*100, delta_tauh_interp[q_mask], label=r'highest $\\tau$')\n",
    "ax.axhline(0, ls='--', c='k', lw=1)\n",
    "\n",
    "ax.set_xlabel(r'% of $\\text{max}(S_{baseline})$ sacrificed')\n",
    "ax.set_ylabel(r'$\\Delta \\tau$ relative to baseline')\n",
    "ax.legend()\n",
    "# ax.set_ylim(-5,10)\n",
    "fig.savefig(fig_prefix + '/tau_minmax_shift.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd5068-af4b-4026-ac8b-3e56322edf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reference indices for per population tau\n",
    "tau_indices = np.arange(tau_sorted.size)\n",
    "\n",
    "for q in [0.0, 0.225, 0.4, 0.6, 0.85]:\n",
    "# for q in [0.052, 0.225, 0.4, 0.6, 0.85]:\n",
    "    # Set the S^* value we're plotting\n",
    "    q_i = np.argmin(np.abs(q_vec - q))\n",
    "    q = q_vec[q_i]\n",
    "\n",
    "    # Get the optimal decision at this {S^*, R} combination\n",
    "    Sstar_rob_i = np.argmin(np.abs(rob_thresh_vec - ((1 - q) * S_opt_baseline)) )\n",
    "    n_opt = int(n_opt_interp[Sstar_rob_i])\n",
    "    l_opt = int(l_opt_interp[Sstar_rob_i])\n",
    "\n",
    "    # Define results vector\n",
    "    results_vector = np.full(tau_sorted.size, np.nan)\n",
    "    # where each population is given a number to indicate optimality under:\n",
    "    #   1 -> baseline condiitons only\n",
    "    #   2 -> baseline and uncertain conditions (risk aversion)\n",
    "    #   3 -> uncertain conditions only\n",
    "    #   0 -> everything else\n",
    "    \n",
    "    # Create relevant masks\n",
    "    baseline_mask = (tau_indices > l_opt_baseline) & (tau_indices < l_opt_baseline + n_opt_baseline)\n",
    "    uncertain_mask = (tau_indices > l_opt) & (tau_indices < l_opt + n_opt)\n",
    "    baseline_only_mask = baseline_mask & (~uncertain_mask)\n",
    "    uncertain_only_mask = uncertain_mask & (~baseline_mask)\n",
    "    both_mask = baseline_mask & uncertain_mask\n",
    "    neither_mask = ~(baseline_mask | uncertain_mask)\n",
    "    \n",
    "    # Use masks to assign values to each population\n",
    "    results_vector[neither_mask] = 0\n",
    "    results_vector[baseline_only_mask] = 1\n",
    "    results_vector[both_mask] = 2\n",
    "    results_vector[uncertain_only_mask] = 3\n",
    "    \n",
    "    # Define colormaping for categories\n",
    "    custom_colors = ['lightgrey', 'coral', 'orchid', 'blueviolet']\n",
    "    labels = ['neither', 'baseline only', 'both', 'uncertain only']\n",
    "    cmap = colors.ListedColormap(custom_colors)\n",
    "    vmin = 0; vmax = len(custom_colors) - 1\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ### TAU DISTRIBUTION VIZ ###\n",
    "    \n",
    "    stack_data = [tau_sorted[results_vector == i] for i in range(len(custom_colors))]\n",
    "    \n",
    "    bins = np.linspace(min(tau_flat), 50, 80)\n",
    "    \n",
    "    # Plot the stacked histogram\n",
    "    fig, ax = plt.subplots(figsize=np.array([8,5])*0.75)\n",
    "    ax.hist(\n",
    "        stack_data,\n",
    "        bins=bins,\n",
    "        stacked=True,\n",
    "        color=custom_colors,\n",
    "        label=[labels[i] for i in range(len(custom_colors))]\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r'$\\tau$')\n",
    "    ax.set_ylabel(r'baseline $\\tau$ frequency')\n",
    "    ax.set_title(f'$q=${np.round(q, 3)}')\n",
    "    ax.legend()\n",
    "    \n",
    "    ### GEOGRAPHICAL MAP ###\n",
    "\n",
    "    mapi_sorted = mapindices[tau_argsort].T\n",
    "\n",
    "    colored_data = np.ones(maps_filt.shape + (4,)) * np.nan #colors in rgba\n",
    "    colored_data[mapi_sorted[0], mapi_sorted[1]] = cmap(norm(results_vector))\n",
    "    # Color background\n",
    "    colored_data[maps_filt == False] = colors.to_rgba('black', alpha=0.3)\n",
    "    # Crop out border where all nans\n",
    "    nonzero_indices = np.nonzero(maps_filt)\n",
    "    row_min, row_max = nonzero_indices[0].min(), nonzero_indices[0].max()\n",
    "    col_min, col_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    colored_data = colored_data[row_min:row_max + 1, col_min:col_max + 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=np.array([10,10])*0.7)\n",
    "    im = ax.imshow(colored_data)#, aspect='auto')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284a755-a68a-47f3-9558-1cf4f94181c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2nd attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaf114-fb56-432c-85d6-9ee645eae68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# First, filter robustness values for this R value\n",
    "rob_all_filtered = rob_all[:, 0, ...]\n",
    "\n",
    "# Unravel the filtered robustness values into y_obs\n",
    "y_obs = rob_all_filtered.ravel()\n",
    "\n",
    "# Place the corresponding decision parameter values into x_obs\n",
    "indices = np.unravel_index(np.arange(y_obs.size), rob_all_filtered.shape)\n",
    "x_obs = np.full((y_obs.size, len(rob_all_filtered.shape)), np.nan)\n",
    "x_obs[:, 0] = rob_thresh_vec[indices[0]]\n",
    "x_obs[:, 1] = ncell_vec[indices[1]]\n",
    "x_obs[:, 2] = slice_left_all[indices[2]]\n",
    "\n",
    "# Filter out any invalid param combinations from both x and y\n",
    "nan_filt = np.isnan(y_obs)\n",
    "# y_obs = y_obs[~nan_filt]\n",
    "# x_obs = x_obs[~nan_filt, :]\n",
    "'''Instead try putting zeros in y_obs for nan'''\n",
    "y_obs[nan_filt] = 0.0\n",
    "\n",
    "# Rescale the y values (i.e. the robustness values)\n",
    "y_rescaler = Rescaler(y_obs.min(axis=0), y_obs.max(axis=0))\n",
    "y_obs = y_rescaler.rescale(y_obs) \n",
    "    \n",
    "# Rescale the x values (i.e. the decision parameters)\n",
    "x_rescaler = Rescaler(x_obs.min(axis=0), x_obs.max(axis=0))\n",
    "x_obs = x_rescaler.rescale(x_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db39fa-bd78-4e5e-b881-0f3491be1fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RBFInterpolator\n",
    "interp = RBFInterpolator(x_obs, y_obs, neighbors=150, smoothing=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3eb5d-1b19-4c2f-b1e3-a4089397c220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rob_thresh_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fc380-a526-4f56-8622-b5a68062e898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the interpolation at a given S^* value\n",
    "# Sstar_i = closest_i_baseline\n",
    "Sstar_i = 99\n",
    "vmin = np.nanmin(rob_all_filtered[Sstar_i])\n",
    "vmax = np.nanmax(rob_all_filtered[Sstar_i])\n",
    "plt.imshow(rob_all_filtered[Sstar_i], vmin=vmin, vmax=vmax, origin='lower')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.show()\n",
    "\n",
    "unraveled_indices = np.unravel_index(np.arange(rob_all_filtered[Sstar_i].size), rob_all_filtered.shape[1:])\n",
    "n_values = ncell_vec[unraveled_indices[0]]\n",
    "l_values = slice_left_all[unraveled_indices[1]]\n",
    "x_test = np.full((rob_all_filtered[Sstar_i].size, 3), np.nan)\n",
    "x_test[:, 0] = rob_thresh_vec[Sstar_i]\n",
    "x_test[:, 1] = n_values\n",
    "x_test[:, 2] = l_values\n",
    "x_test = x_rescaler.rescale(x_test)\n",
    "\n",
    "y_test = interp(x_test)\n",
    "y_test = y_rescaler.descale(y_test)\n",
    "y_imshow = y_test.reshape((rob_all_filtered[Sstar_i].shape))\n",
    "plt.imshow(y_imshow, vmin=vmin, vmax=vmax, origin='lower')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.show()\n",
    "\n",
    "new_nan_filt = np.isnan(rob_all_filtered[Sstar_i].ravel())\n",
    "print(f'raw data (min, max)=({vmin, vmax})')\n",
    "interp_min, interp_max = (np.min(y_test[~new_nan_filt]), np.max(y_test[~new_nan_filt]))\n",
    "print(f'interp data (min, max)=({interp_min, interp_max})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1e0bd-4d1f-4d41-bed6-131ea0aa537f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get parameters and S values under baseline condintions\n",
    "C_mask = (x_all[:, 0] == (C*ncell_tot))\n",
    "baseline_mask = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "x_obs_baseline = x_all[C_mask & baseline_mask, 1:3]\n",
    "y_obs_baseline = meta_metric_all[C_mask & baseline_mask]\n",
    "\n",
    "# Rescale inputs and outputs\n",
    "x_rescaler_baseline = Rescaler(x_obs_baseline.min(axis=0), x_obs_baseline.max(axis=0))\n",
    "x_obs_baseline = x_rescaler_baseline.rescale(x_obs_baseline)\n",
    "y_rescaler_baseline = Rescaler(y_obs_baseline.min(axis=0), y_obs_baseline.max(axis=0))\n",
    "y_obs_baseline = y_rescaler_baseline.rescale(y_obs_baseline) \n",
    "\n",
    "# Interpolate S(n, l) given R under baseline conditions for reference during optimization\n",
    "interp_baseline = RBFInterpolator(x_obs_baseline, y_obs_baseline, neighbors=100, smoothing=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600d2cd-316c-4007-a8b2-0193c909b876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define objective function (i.e. robustness) to be optimized\n",
    "def objective(decision_params, *args):\n",
    "    Sstar = args[0]\n",
    "    \n",
    "    S_baseline = interp_baseline([decision_params])\n",
    "    if S_baseline < Sstar:\n",
    "        # Only consider robustness to be nonzero if S^* met under baseline\n",
    "        robustness = 0\n",
    "    else:\n",
    "        # Take robustness value from interpolation\n",
    "        x = np.full(len(decision_params)+1, np.nan)\n",
    "        x[0] = Sstar\n",
    "        x[1:] = decision_params\n",
    "        robustness = interp([x])\n",
    "    \n",
    "    return -robustness # Negate bc using minimization algorithm\n",
    "\n",
    "# Now step through S^* values and find decisions that optimize robustness\n",
    "# for Sstar_i, Sstar in enumerate(rob_thresh_vec):\n",
    "for Sstar_i, Sstar in zip([Sstar_i], [rob_thresh_vec[Sstar_i]]):\n",
    "    # Use optimal decision from exisiting samples as starting point\n",
    "    x0_position = np.nanargmax(rob_all_filtered[Sstar_i, :])\n",
    "    n0_i, l0_i = np.unravel_index(x0_position, rob_all_filtered.shape[1:])\n",
    "    n0, l0 = (ncell_vec[n0_i], slice_left_all[l0_i])\n",
    "    \n",
    "    # Rescale to interpolation scale\n",
    "    Sstar, n0, l0 = x_rescaler.rescale([Sstar, n0, l0])\n",
    "\n",
    "    # Use an optimizer that can handle some noise in the objective\n",
    "    x0 = np.array([n0, l0])\n",
    "    bounds = ((0, 1), (0, 1)) # Remeber, we rescaled the training data\n",
    "    cons = [{'type': 'ineq', 'fun': lambda x:  1 - x[1] - x[0]}] # Constrain l < (n_tot - n)\n",
    "    res = minimize(objective, x0, args=(Sstar), method='COBYLA', bounds=bounds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59786075-1b87-4d79-a9a2-0bd5b6a64eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'original optimum of {np.nanmax(rob_all_filtered[Sstar_i])} at {x_rescaler.descale([Sstar_i, x0[0], x0[1]])}')\n",
    "print(f'interpolated optimum of {y_rescaler.descale(-res.fun)} at {x_rescaler.descale([Sstar_i, res.x[0], res.x[1]])}')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afde24-45e3-4e2d-a84c-81854ddabe95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = np.nanmin(rob_all_filtered[Sstar_i])\n",
    "vmax = np.nanmax(rob_all_filtered[Sstar_i])\n",
    "plt.imshow(rob_all_filtered[Sstar_i], vmin=vmin, vmax=vmax, origin='lower')\n",
    "\n",
    "descaled_orignal = x_rescaler.descale([Sstar_i, x0[0], x0[1]])[1:]\n",
    "descaled_interp = x_rescaler.descale([Sstar_i, res.x[0], res.x[1]])[1:]\n",
    "labels = ['raw', 'interpolated']\n",
    "for i, [n_opt, l_opt] in enumerate([descaled_orignal, descaled_interp]):\n",
    "    # min + (i / length) * (max - min) = (value at i); solve for i\n",
    "    n_opt_i = (n_opt - x_rescaler.mins[1]) * (len(ncell_vec) / (x_rescaler.maxes[1] - x_rescaler.mins[1]))\n",
    "    l_opt_i = (l_opt - x_rescaler.mins[2]) * (len(slice_left_all) / (x_rescaler.maxes[2] - x_rescaler.mins[2]))\n",
    "    plt.scatter(l_opt_i, n_opt_i, marker='x', s=120, linewidths=2.2, label=labels[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(y_imshow, vmin=vmin, vmax=vmax, origin='lower')\n",
    "for i, [n_opt, l_opt] in enumerate([descaled_orignal, descaled_interp]):\n",
    "    # min + (i / length) * (max - min) = (value at i); solve for i\n",
    "    n_opt_i = (n_opt - x_rescaler.mins[1]) * (len(ncell_vec) / (x_rescaler.maxes[1] - x_rescaler.mins[1]))\n",
    "    l_opt_i = (l_opt - x_rescaler.mins[2]) * (len(slice_left_all) / (x_rescaler.maxes[2] - x_rescaler.mins[2]))\n",
    "    plt.scatter(l_opt_i, n_opt_i, marker='x', s=120, linewidths=2.2, label=labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a381ed-d845-4aba-8d75-1ff092fd8490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1st attempt (by way of $S(R, n, l, ...)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7ae9f-c8ec-4b9f-b098-947463da05cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.interpolate import RBFInterpolator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# import faiss\n",
    "\n",
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# Filter parameter and metric metric values into xobs and yobs, respectively\n",
    "indices = np.nonzero(x_all[:, 0] == (C*ncell_tot))[0]\n",
    "x_obs = x_all[indices, 1:]\n",
    "y_obs = meta_metric_all[indices]\n",
    "\n",
    "# Class to rescale inputs and outputs to [0,1] for numerical stability\n",
    "# Also store descalers to interpret interpolated values\n",
    "class Rescaler:\n",
    "    def __init__(self, mins, maxes):\n",
    "        \"\"\"\n",
    "        mins: vector of minima\n",
    "        maxes: vector of maxima\n",
    "        \"\"\"\n",
    "        self.mins = mins\n",
    "        self.maxes = maxes\n",
    "\n",
    "    def rescale(self, x):\n",
    "        return (x - self.mins) / (self.maxes - self.mins)\n",
    "    \n",
    "    def descale(self, x):\n",
    "        return (x * (self.maxes - self.mins)) + self.mins\n",
    "    \n",
    "x_rescaler = Rescaler(x_obs.min(axis=0), x_obs.max(axis=0))\n",
    "x_obs = x_rescaler.rescale(x_obs)\n",
    "\n",
    "y_rescaler = Rescaler(y_obs.min(), y_obs.max())\n",
    "y_obs = y_rescaler.rescale(y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26709a28-6b2f-4a7e-916a-320f255e4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the interpolator\n",
    "# interp = RBFInterpolator(x_obs, y_obs, neighbors=50)\n",
    "\n",
    "interp = KNeighborsRegressor(n_neighbors=100, weights='distance', algorithm='auto', n_jobs=6)\n",
    "interp.fit(x_obs, y_obs)\n",
    "\n",
    "# d = x_obs.shape[1]\n",
    "# index = faiss.IndexFlatL2(d)\n",
    "# index.add(x_obs.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac493f93-2e64-4d98-b507-e9792473062c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of uncertain param samples to evaluate robustness with\n",
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'demographic_index']\n",
    "num_points = 6\n",
    "linspaces = [np.linspace(0, 1, num_points) for _ in range(5)] # Still in rescaled space so all [0,1]\n",
    "grid_arrays = np.meshgrid(*linspaces)\n",
    "uncertainty_samples = np.vstack([array.ravel() for array in grid_arrays]).T\n",
    "\n",
    "# Initialize set of full samples for which the decision parameters will be filled in\n",
    "empty_samples = np.full((len(uncertainty_samples), 2), np.nan)\n",
    "full_samples = np.column_stack((empty_samples, uncertainty_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83b602-7fa3-4ebb-bd3c-10afec32e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate at an example point\n",
    "n = 0.5\n",
    "l = 0.2\n",
    "\n",
    "full_samples[:, 0] = n\n",
    "full_samples[:, 1] = l\n",
    "\n",
    "# interp(full_samples)\n",
    "\n",
    "interp.predict(full_samples)\n",
    "\n",
    "# k = 50  # Number of neighbors\n",
    "# distances, indices = index.search(full_samples.astype('float32'), k)\n",
    "# Y_neighbors = y_obs[indices]  # shape: (n_query, k)\n",
    "# y_pred = Y_neighbors.mean(axis=1)  # shape: (n_query,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb34d8-c594-4947-b9e1-cf3f6fbc4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize at a particular S^* value\n",
    "Sstar = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a309cb5-15d0-4454-adc2-b7e3abf58edd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### RBF example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913c586-f256-4382-8e1b-5718df61bcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.stats.qmc import Halton\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "xobs = 2*Halton(2, seed=rng).random(100) - 1\n",
    "yobs = np.sum(xobs, axis=1)*np.exp(-6*np.sum(xobs**2, axis=1))\n",
    "\n",
    "xgrid = np.mgrid[-1:1:50j, -1:1:50j]\n",
    "xflat = xgrid.reshape(2, -1).T\n",
    "yflat = RBFInterpolator(xobs, yobs)(xflat)\n",
    "ygrid = yflat.reshape(50, 50)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(*xgrid, ygrid, vmin=-0.25, vmax=0.25, shading='gouraud')\n",
    "p = ax.scatter(*xobs.T, c=yobs, s=50, ec='k', vmin=-0.25, vmax=0.25)\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
