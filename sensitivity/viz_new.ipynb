{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abf5dc-f407-4182-a9e5-f31979ab41a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import pickle\n",
    "import signac as sg\n",
    "from scipy.special import gamma\n",
    "import copy as copy\n",
    "import scipy\n",
    "from global_functions import adjustmaps\n",
    "import h5py\n",
    "from scipy.interpolate import make_lsq_spline\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# Define/load things non-specific to a given set of results\n",
    "metric = 'lambda_s'\n",
    "Aeff = 7.29\n",
    "t_final = 300\n",
    "ncell_tot = 87_993\n",
    "c = 1.42\n",
    "with sg.H5Store('shared_data.h5').open(mode='r') as sd:\n",
    "    b_vec = np.array(sd['b_vec'])\n",
    "tau_vec = b_vec * gamma(1+1/c)\n",
    "tauc_methods = [\"flat\"]\n",
    "results_pre = 'gte_thresh' \n",
    "\n",
    "# Update global plotting parameters\n",
    "rc('axes', labelsize=21)  # Font size for x and y labels\n",
    "rc('axes', titlesize=16)\n",
    "rc('xtick', labelsize=19)  # Font size for x-axis tick labels\n",
    "rc('ytick', labelsize=19)  # Font size for y-axis tick labels\n",
    "rc('lines', markersize=15)  \n",
    "rc('lines', linewidth=5.5)\n",
    "rc('legend', fontsize=19)\n",
    "rc('font', family='sans-serif')\n",
    "rc('font', serif=['Computer Modern Sans Serif'] + plt.rcParams['font.serif'])\n",
    "rc('font', weight='light')\n",
    "histlw = 5.5\n",
    "cbar_lpad = 30\n",
    "dpi = 50\n",
    "# dpi = 200\n",
    "\n",
    "# Function to read in things specific to given results as global variables\n",
    "def set_globals(results_pre):\n",
    "    if metric == 'lambda_s':\n",
    "        globals()['metric_lab'] = r'$S$'\n",
    "        globals()['rob_metric_lab'] = r'$S^*$'\n",
    "        globals()['mean_metric_lab'] = r'$\\bar{\\lambda}(\\tau)$'\n",
    "    if metric == 'P_s':\n",
    "        globals()['metric_lab'] = r'$S_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\S_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<P_s>$'\n",
    "    if metric == 's':\n",
    "        globals()['metric_lab'] = r'$s_{meta}$'\n",
    "        globals()['rob_metric_lab'] = r'$\\s_{meta}^*$'\n",
    "        globals()['mean_metric_lab'] = r'$<s>$'\n",
    "    globals()['fn_prefix'] = f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    #globals()['fig_prefix'] = f\"{results_pre}/figs/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/\"\n",
    "    globals()['fig_prefix'] = os.path.join('/','Volumes', 'Macintosh HD', 'Users', 'patrick', \n",
    "                                           'Google Drive', 'My Drive', 'Research', 'Regan', 'Figs/')\n",
    "\n",
    "    # Load things saved specific to these results\n",
    "    globals()['metric_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/metric_{metric}/metric_all.npy\")\n",
    "    globals()['tau_all'] = np.load(f\"{results_pre}/data/Aeff_{Aeff}/tfinal_{t_final}/tau_all.npy\")\n",
    "    globals()['C_vec'] = np.load(fn_prefix + \"C_vec.npy\")\n",
    "    globals()['C_i_vec'] = np.arange(len(C_vec))[::2]\n",
    "    globals()['ncell_vec'] = np.load(fn_prefix + \"ncell_vec.npy\")\n",
    "    globals()['slice_left_all'] = np.load(fn_prefix + \"slice_left_all.npy\")\n",
    "    eps_axes = {}\n",
    "    with h5py.File(fn_prefix + \"eps_axes.h5\", \"r\") as handle:\n",
    "        for key in handle.keys():\n",
    "            eps_axes.update({key: handle[key][()]})\n",
    "    globals()['eps_axes'] = eps_axes\n",
    "\n",
    "# Read in maps and convert fdm to tau, used by multiple plots below\n",
    "ul_coord = [1500, 2800]\n",
    "lr_coord = [2723, 3905]\n",
    "usecols = np.arange(ul_coord[0],lr_coord[0])\n",
    "sdmfn = \"../shared_maps/SDM_1995.asc\"\n",
    "sdm = np.loadtxt(sdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "fdmfn = '../shared_maps/FDE_current_allregions.asc'\n",
    "fdm = np.loadtxt(fdmfn,skiprows=6+ul_coord[1],\n",
    "                         max_rows=lr_coord[1], usecols=usecols)\n",
    "sdm, fdm = adjustmaps([sdm, fdm])\n",
    "delta_t = 30\n",
    "b_raster = delta_t / np.power(-np.log(1-fdm), 1/c)\n",
    "tau_raster = b_raster * gamma(1+1/c)\n",
    "maps_filt = (sdm > 0) & (fdm > 0)\n",
    "tau_flat = tau_raster[maps_filt] \n",
    "mapindices = np.argwhere(maps_filt)\n",
    "tau_argsort = np.argsort(tau_flat)\n",
    "tau_sorted = tau_flat[tau_argsort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245b13c-6547-4195-82eb-5312a29bf5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_globals(results_pre)\n",
    "x_all = np.load(fn_prefix + '/x_all.npy')\n",
    "meta_metric_all = np.load(fn_prefix + '/meta_metric_all.npy')\n",
    "meta_metric_all = meta_metric_all[:,0]\n",
    "\n",
    "# Create a filter for the baseline scenario\n",
    "zero_eps_mask = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "\n",
    "maxrob = np.load(fn_prefix + \"maxrob.npy\")\n",
    "argmaxrob = np.load(fn_prefix + \"argmaxrob.npy\")\n",
    "rob_thresh_vec = np.load(fn_prefix + \"rob_thresh_vec.npy\")\n",
    "rob_all = np.load(fn_prefix + \"rob_all.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b7c12-24ff-470d-9568-a03959eb8061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Looking at raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218fabf-9e87-405a-8102-798bc790c53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# q_vec = np.arange(0.1, 0.9, 0.1)\n",
    "q_vec = np.arange(0.05, 0.85, 0.05)\n",
    "\n",
    "# results = {}\n",
    "keys = ['delta_range', 'delta_median', 'median', 'range', 'delta_n', 'delta_l', 'n', 'l',\n",
    "        'delta_taul', 'delta_tauh']\n",
    "results = {key: np.full((C_vec.size, q_vec.size), np.nan) for key in keys}\n",
    "results['n_baseline'] = np.full(C_vec.size, np.nan)\n",
    "results['l_baseline'] = np.full(C_vec.size, np.nan)\n",
    "\n",
    "for C_i, C in enumerate(C_vec):\n",
    "    for q_i, q in enumerate(q_vec):\n",
    "        if q_i == 0:\n",
    "            # First get optimal tau slice stats from optimal decisions under baseline\n",
    "            zeroeps_filt = np.all(x_all[:, 3:] == 0, axis=1)\n",
    "            _filt = zeroeps_filt & (x_all[:,0] == C)\n",
    "            argmax = np.nanargmax(meta_metric_all[_filt])\n",
    "            S_opt_baseline = meta_metric_all[_filt][argmax]\n",
    "            n_opt_baseline, sl_opt_baseline = x_all[_filt,:][argmax][1:3].astype(int)\n",
    "            tau_slice_baseline = tau_sorted[sl_opt_baseline:sl_opt_baseline+n_opt_baseline]\n",
    "            range_baseline = tau_slice_baseline.max() - tau_slice_baseline.min()\n",
    "            median_baseline = np.median(tau_slice_baseline)\n",
    "            \n",
    "            results['n_baseline'][C_i] = n_opt_baseline\n",
    "            results['l_baseline'][C_i] = sl_opt_baseline\n",
    "\n",
    "        # Now get the same statistics for (1-q) * optimal S baseline\n",
    "        Sstar_rob_i = np.argmin(np.abs(rob_thresh_vec - ((1 - q) * S_opt_baseline)) )\n",
    "        n_opt_rob = ncell_vec[int(argmaxrob[Sstar_rob_i, C_i][0])]\n",
    "        sl_opt_rob = slice_left_all[int(argmaxrob[Sstar_rob_i, C_i][1])]\n",
    "        tau_slice_rob = tau_sorted[sl_opt_rob:sl_opt_rob+n_opt_rob]\n",
    "        range_rob = tau_slice_rob.max() - tau_slice_rob.min()\n",
    "        median_rob = np.median(tau_slice_rob)\n",
    "\n",
    "        results['delta_median'][C_i, q_i] = median_rob - median_baseline\n",
    "        results['delta_range'][C_i, q_i] = range_rob - range_baseline\n",
    "        results['median'][C_i, q_i] = median_rob\n",
    "        results['range'][C_i, q_i] = range_rob\n",
    "        results['delta_n'][C_i, q_i] = n_opt_rob - n_opt_baseline\n",
    "        results['delta_l'][C_i, q_i] = sl_opt_rob - sl_opt_baseline\n",
    "        results['n'][C_i, q_i] = n_opt_rob\n",
    "        results['l'][C_i, q_i] = sl_opt_rob\n",
    "        results['delta_taul'][C_i][q_i] = tau_sorted[sl_opt_rob] - tau_sorted[sl_opt_baseline]\n",
    "        results['delta_tauh'][C_i][q_i] = tau_sorted[sl_opt_rob+n_opt_rob] - tau_sorted[sl_opt_baseline+n_opt_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafc243-3af9-4566-a678-79c6822d7977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results['delta_l+delta_n'] = (results['delta_l'] + results['delta_n']) #/ np.abs(results['delta_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860096cd-3e87-4f83-8f8d-4f4ff1808638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define reference indices for per population tau\n",
    "tau_indices = np.arange(tau_sorted.size)\n",
    "\n",
    "# Set the R value we're plotting\n",
    "C = 9\n",
    "assert C in C_vec/ncell_tot\n",
    "C_i = (C_vec/ncell_tot == C).argmax()\n",
    "# print(f\"at baseline for C={C_vec[C_i]} \\n optimal n,l = {results['n_baseline'][C_i], results['l_baseline'][C_i]} \\n\")\n",
    "\n",
    "for q in [0.05, 0.45, 0.8]:\n",
    "# for q in [0.5]:\n",
    "    # Set the S^* value we're plotting\n",
    "    assert np.any(np.isclose(q_vec, q))\n",
    "    q_i = np.isclose(q_vec, q).argmax()\n",
    "\n",
    "    # Get the optimal decision at this {S^*, R} combination\n",
    "    n_opt = results['n'][C_i, q_i]\n",
    "    l_opt = results['l'][C_i, q_i]\n",
    "    # print(f'at q={q}\\n optimal n,l = {n_opt, l_opt} \\n')\n",
    "\n",
    "    # Define results vector\n",
    "    results_vector = np.full(tau_sorted.size, np.nan)\n",
    "    # where each population is given a number to indicate optimality under:\n",
    "    #   1 -> baseline condiitons only\n",
    "    #   2 -> baseline and uncertain conditions (risk aversion)\n",
    "    #   3 -> uncertain conditions only\n",
    "    #   0 -> everything else\n",
    "    \n",
    "    # Create relevant masks\n",
    "    baseline_mask = (tau_indices > results['l_baseline'][C_i]) & (tau_indices < results['l_baseline'][C_i] + results['n_baseline'][C_i])\n",
    "    uncertain_mask = (tau_indices > l_opt) & (tau_indices < l_opt + n_opt)\n",
    "    baseline_only_mask = baseline_mask & (~uncertain_mask)\n",
    "    uncertain_only_mask = uncertain_mask & (~baseline_mask)\n",
    "    both_mask = baseline_mask & uncertain_mask\n",
    "    neither_mask = ~(baseline_mask | uncertain_mask)\n",
    "    \n",
    "    # Use masks to assign values to each population\n",
    "    results_vector[neither_mask] = 0\n",
    "    results_vector[baseline_only_mask] = 1\n",
    "    results_vector[both_mask] = 2\n",
    "    results_vector[uncertain_only_mask] = 3\n",
    "    \n",
    "    # Define colormaping for categories\n",
    "    custom_colors = ['lightgrey', 'coral', 'orchid', 'blueviolet']\n",
    "    labels = ['neither', 'baseline only', 'both', 'uncertain only']\n",
    "    cmap = colors.ListedColormap(custom_colors)\n",
    "    vmin = 0; vmax = len(custom_colors) - 1\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ### TAU DISTRIBUTION VIZ ###\n",
    "    \n",
    "    stack_data = [tau_sorted[results_vector == i] for i in range(len(custom_colors))]\n",
    "    \n",
    "    bins = np.linspace(min(tau_flat), 50, 80)\n",
    "    \n",
    "    # Plot the stacked histogram\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.hist(\n",
    "        stack_data,\n",
    "        bins=bins,\n",
    "        stacked=True,\n",
    "        color=custom_colors,\n",
    "        label=[labels[i] for i in range(len(custom_colors))]\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r'$\\tau$')\n",
    "    ax.set_ylabel(r'baseline $\\tau$ frequency')\n",
    "    ax.set_title(f'$q=${q}')\n",
    "    ax.legend()\n",
    "    \n",
    "    ### GEOGRAPHICAL MAP ###\n",
    "\n",
    "    mapi_sorted = mapindices[tau_argsort].T\n",
    "\n",
    "    colored_data = np.ones(maps_filt.shape + (4,)) * np.nan #colors in rgba\n",
    "    colored_data[mapi_sorted[0], mapi_sorted[1]] = cmap(norm(results_vector))\n",
    "    # Color background\n",
    "    colored_data[maps_filt == False] = colors.to_rgba('black', alpha=0.3)\n",
    "    # Crop out border where all nans\n",
    "    nonzero_indices = np.nonzero(maps_filt)\n",
    "    row_min, row_max = nonzero_indices[0].min(), nonzero_indices[0].max()\n",
    "    col_min, col_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    colored_data = colored_data[row_min:row_max + 1, col_min:col_max + 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(colored_data)#, aspect='auto')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1604aa-df02-43dc-8356-7cddff054d6a",
   "metadata": {},
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2699d-7027-446d-95c3-aeccc17b4574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [r'$\\Delta(\\tau~\\text{range})$', r'$\\Delta(\\tau~\\text{median})$', \n",
    "          r'$\\tau~\\text{range}$', r'$\\tau~\\text{median}$',\n",
    "          r'$\\Delta l + \\Delta n$',\n",
    "          r'$\\Delta n$', r'$\\Delta l$', r'$n$', r'$l$',\n",
    "          r'$\\Delta \\tau_l$', r'$\\Delta \\tau_h$']\n",
    "# cmap = copy.copy(cm.RdPu_r)\n",
    "# cmap = copy.copy(cm.Wistia)\n",
    "cmap = copy.copy(cm.cool)\n",
    "vmin = C_vec.min() if (len(C_vec) > 1) else 0\n",
    "norm = colors.Normalize(vmin=vmin/ncell_tot, vmax=C_vec.max()/ncell_tot)\n",
    "\n",
    "# Restrict the range of plotting to a desired q value\n",
    "q_lim = 0.85\n",
    "q_mask = q_vec <= q_lim \n",
    "\n",
    "for key_i, key in enumerate(['delta_range', 'delta_median', 'range', 'median',\n",
    "                             'delta_l+delta_n', 'delta_n', 'delta_l', 'n', 'l',\n",
    "                             'delta_taul', 'delta_tauh']):\n",
    "    fig, ax = plt.subplots(figsize=(9,5))\n",
    "    \n",
    "    for C_i, C in enumerate(C_vec):\n",
    "        # if (C_i) % 2 == 0: continue\n",
    "        # if C_i < 3: continue\n",
    "        # if C_i > 3: continue\n",
    "        # if C_i != len(C_vec) - 1: continue\n",
    "        # if (C/ncell_tot) < 9: continue\n",
    "        # if (C/ncell_tot) != 5: continue\n",
    "        color = cmap(norm(C/ncell_tot))\n",
    "        ax.scatter(q_vec[q_mask]*100, results[key][C_i, q_mask], marker='o', c=color, alpha=0.8)\n",
    "        \n",
    "    # ax.set_xlabel(r'$q$; $S^*=(1-q)\\text{max}(S_{baseline})$')\n",
    "    ax.set_xlabel(r'% of $\\text{max}(S_{baseline})$ sacrificed')\n",
    "    ax.set_ylabel(labels[key_i])\n",
    "    \n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array(C_vec)\n",
    "    cbar = fig.colorbar(sm, label=r'$R~/~n_{tot}$', ax=ax)\n",
    "    \n",
    "    if key not in ['range', 'median', 'n', 'l']:\n",
    "        ax.axhline(0, ls='--', lw=1, c='k')\n",
    "    # ax.set_xlim(0.025,0.525)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d895f-f1ad-4aef-9fb8-6000b1db653f",
   "metadata": {},
   "source": [
    "### Interpolate $\\omega$ by way of $S(R, n, l, ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284a755-a68a-47f3-9558-1cf4f94181c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2nd attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaf114-fb56-432c-85d6-9ee645eae68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# First, filter robustness values for this R value\n",
    "...\n",
    "\n",
    "# Unravel the filtered robustness values into y_obs\n",
    "...\n",
    "\n",
    "# Get the corresponding decision parameter values for each sample\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db39fa-bd78-4e5e-b881-0f3491be1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to rescale inputs and outputs to [0,1] for numerical stability\n",
    "# Also store descalers to interpret interpolated values\n",
    "class Rescaler:\n",
    "    def __init__(self, mins, maxes):\n",
    "        \"\"\"\n",
    "        mins: vector of minima\n",
    "        maxes: vector of maxima\n",
    "        \"\"\"\n",
    "        self.mins = mins\n",
    "        self.maxes = maxes\n",
    "\n",
    "    def rescale(self, x):\n",
    "        return (x - self.mins) / (self.maxes - self.mins)\n",
    "    \n",
    "    def descale(self, x):\n",
    "        return (x * (self.maxes - self.mins)) + self.mins\n",
    "    \n",
    "# Rescale the y values (i.e. the robustness values)\n",
    "y_rescaler = Rescaler(y_obs.min(axis=0), y_obs.max(axis=0))\n",
    "y_obs = x_rescaler.rescale(y_obs) \n",
    "    \n",
    "# Rescale the x values (i.e. the decision parameters)\n",
    "x_rescaler = Rescaler(x_obs.min(axis=0), x_obs.max(axis=0))\n",
    "x_obs = x_rescaler.rescale(x_obs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e64a61-8dc6-4a89-aed9-f10acb1a141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now interpolate the robustness as a function of decision parameters and\n",
    "# stability threshold S^*, using an interpolator that can handle noise\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a381ed-d845-4aba-8d75-1ff092fd8490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1st attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7ae9f-c8ec-4b9f-b098-947463da05cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.interpolate import RBFInterpolator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# import faiss\n",
    "\n",
    "C = 9\n",
    "assert np.any(np.isclose(C_vec/ncell_tot, C))\n",
    "C_i = np.isclose(C_vec/ncell_tot, C).argmax()\n",
    "\n",
    "# Filter parameter and metric metric values into xobs and yobs, respectively\n",
    "indices = np.nonzero(x_all[:, 0] == (C*ncell_tot))[0]\n",
    "x_obs = x_all[indices, 1:]\n",
    "y_obs = meta_metric_all[indices]\n",
    "\n",
    "# Class to rescale inputs and outputs to [0,1] for numerical stability\n",
    "# Also store descalers to interpret interpolated values\n",
    "class Rescaler:\n",
    "    def __init__(self, mins, maxes):\n",
    "        \"\"\"\n",
    "        mins: vector of minima\n",
    "        maxes: vector of maxima\n",
    "        \"\"\"\n",
    "        self.mins = mins\n",
    "        self.maxes = maxes\n",
    "\n",
    "    def rescale(self, x):\n",
    "        return (x - self.mins) / (self.maxes - self.mins)\n",
    "    \n",
    "    def descale(self, x):\n",
    "        return (x * (self.maxes - self.mins)) + self.mins\n",
    "    \n",
    "x_rescaler = Rescaler(x_obs.min(axis=0), x_obs.max(axis=0))\n",
    "x_obs = x_rescaler.rescale(x_obs)\n",
    "\n",
    "y_rescaler = Rescaler(y_obs.min(), y_obs.max())\n",
    "y_obs = y_rescaler.rescale(y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26709a28-6b2f-4a7e-916a-320f255e4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the interpolator\n",
    "# interp = RBFInterpolator(x_obs, y_obs, neighbors=50)\n",
    "\n",
    "interp = KNeighborsRegressor(n_neighbors=100, weights='distance', algorithm='auto', n_jobs=6)\n",
    "interp.fit(x_obs, y_obs)\n",
    "\n",
    "# d = x_obs.shape[1]\n",
    "# index = faiss.IndexFlatL2(d)\n",
    "# index.add(x_obs.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac493f93-2e64-4d98-b507-e9792473062c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of uncertain param samples to evaluate robustness with\n",
    "uncertain_params = ['mu_tau', 'sigm_tau', 'mu_tauc', 'sigm_tauc', 'demographic_index']\n",
    "num_points = 6\n",
    "linspaces = [np.linspace(0, 1, num_points) for _ in range(5)] # Still in rescaled space so all [0,1]\n",
    "grid_arrays = np.meshgrid(*linspaces)\n",
    "uncertainty_samples = np.vstack([array.ravel() for array in grid_arrays]).T\n",
    "\n",
    "# Initialize set of full samples for which the decision parameters will be filled in\n",
    "empty_samples = np.full((len(uncertainty_samples), 2), np.nan)\n",
    "full_samples = np.column_stack((empty_samples, uncertainty_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83b602-7fa3-4ebb-bd3c-10afec32e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate at an example point\n",
    "n = 0.5\n",
    "l = 0.2\n",
    "\n",
    "full_samples[:, 0] = n\n",
    "full_samples[:, 1] = l\n",
    "\n",
    "# interp(full_samples)\n",
    "\n",
    "interp.predict(full_samples)\n",
    "\n",
    "# k = 50  # Number of neighbors\n",
    "# distances, indices = index.search(full_samples.astype('float32'), k)\n",
    "# Y_neighbors = y_obs[indices]  # shape: (n_query, k)\n",
    "# y_pred = Y_neighbors.mean(axis=1)  # shape: (n_query,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb34d8-c594-4947-b9e1-cf3f6fbc4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize at a particular S^* value\n",
    "Sstar = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a309cb5-15d0-4454-adc2-b7e3abf58edd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### RBF example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913c586-f256-4382-8e1b-5718df61bcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.stats.qmc import Halton\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "xobs = 2*Halton(2, seed=rng).random(100) - 1\n",
    "yobs = np.sum(xobs, axis=1)*np.exp(-6*np.sum(xobs**2, axis=1))\n",
    "\n",
    "xgrid = np.mgrid[-1:1:50j, -1:1:50j]\n",
    "xflat = xgrid.reshape(2, -1).T\n",
    "yflat = RBFInterpolator(xobs, yobs)(xflat)\n",
    "ygrid = yflat.reshape(50, 50)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(*xgrid, ygrid, vmin=-0.25, vmax=0.25, shading='gouraud')\n",
    "p = ax.scatter(*xobs.T, c=yobs, s=50, ec='k', vmin=-0.25, vmax=0.25)\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
